[
  {
    "path": ".",
    "files": [
      {
        "name": ".DS_Store",
        "error": "Binary file not decoded"
      },
      {
        "name": ".env",
        "content": "# Environment\nAPP_ENV=development\n\n# Database\nDB_TYPE=postgresql\nDB_NAME=ai_orchestrator\nDB_HOST=localhost\nDB_PORT=5432\nDB_USER=postgres\nDB_PASSWORD=IknowNothing\nDB_SCHEMA=public\n\n# Database Pool Settings\nDB_POOL_SIZE=5\nDB_MAX_OVERFLOW=10\nDB_POOL_TIMEOUT=30\nDB_POOL_RECYCLE=1800\n\n# LM Studio Configuration\nLMSTUDIO_BASE_URL=http://localhost:1234/v1\n\n# Model Configuration\nATLAS_MODEL=lmstudio-community/Phi-3.1-mini-128k-instruct-GGUF/Phi-3.1-mini-128k-instruct-Q4_K_M.gguf\nNOVA_MODEL=lmstudio-community/Phi-3.1-mini-128k-instruct-GGUF/Phi-3.1-mini-128k-instruct-Q4_K_M.gguf:2\nSAGE_MODEL=lmstudio-community/Phi-3.1-mini-128k-instruct-GGUF/Phi-3.1-mini-128k-instruct-Q4_K_M.gguf:3\nECHO_MODEL=lmstudio-community/Phi-3.1-mini-128k-instruct-GGUF/Phi-3.1-mini-128k-instruct-Q4_K_M.gguf:4\nPIXEL_MODEL=lmstudio-community/Phi-3.1-mini-128k-instruct-GGUF/Phi-3.1-mini-128k-instruct-Q4_K_M.gguf:5\nQUANTUM_MODEL=lmstudio-community/Phi-3.1-mini-128k-instruct-GGUF/Phi-3.1-mini-128k-instruct-Q4_K_M.gguf:6\n\n# Service Configuration\nATLAS_PORT=8000\nNOVA_PORT=8001\nECHO_PORT=8002\nPIXEL_PORT=8003\nSAGE_PORT=8004\nQUANTUM_PORT=8005\n\n# RabbitMQ Configuration\nRABBITMQ_HOST=localhost\nRABBITMQ_PORT=5672\nRABBITMQ_USER=guest\nRABBITMQ_PASSWORD=guest\nRABBITMQ_VHOST=/\n\n# Logging Configuration\nLOG_LEVEL=INFO\nLOG_FORMAT=detailed  # basic, detailed, or json\nLOG_TO_FILE=false\nLOG_FILE_PATH=logs/ai_orchestrator.log\n\n# Timeout Settings\nREQUEST_TIMEOUT=240\nREFLECTION_DEPTH=2\nMAX_RETRIES=3\n\n# System Metrics\nENABLE_METRICS=true\nMETRICS_PORT=9090",
        "size": 1573
      },
      {
        "name": ".gitattributes",
        "content": "# Auto detect text files and perform LF normalization\n* text=auto\n",
        "size": 66
      },
      {
        "name": ".gitignore",
        "content": "\n/ai-orchestration\n",
        "size": 19
      },
      {
        "name": "20241211handover.txt",
        "error": "Binary file not decoded"
      },
      {
        "name": "alembic.ini",
        "content": "[alembic]\nscript_location = migrations\nsqlalchemy.url = postgresql+asyncpg://postgres:IknowNothing@localhost:5432/ai_orchestrator\n\n[loggers]\nkeys = root,sqlalchemy,alembic\n\n[handlers]\nkeys = console\n\n[formatters]\nkeys = generic\n\n[logger_root]\nlevel = WARN\nhandlers = console\nqualname = \n\n[logger_sqlalchemy]\nlevel = WARN\nhandlers = \nqualname = sqlalchemy.engine\n\n[logger_alembic]\nlevel = INFO\nhandlers = \nqualname = alembic\n\n[handler_console]\nclass = StreamHandler\nargs = (sys.stderr,)\nlevel = NOTSET\nformatter = generic\n\n[formatter_generic]\nformat = %(levelname)-5.5s [%(name)s] %(message)s\ndatefmt = %H:%M:%S\n\n",
        "size": 650
      },
      {
        "name": "exportcodebase.py",
        "content": "import os\nimport json\nfrom pathlib import Path\n\ndef export_codebase(directory=\"./\", output_dir=\"./codebase_export\"):\n    \"\"\"\n    Exports folder structure and file contents in Spaces-compatible format\n    Returns: Path to output directory\n    \"\"\"\n    print(\"Starting export...\")  # Debug print\n    output = []\n    excluded_dirs = {'__pycache__', 'venv', '.git'}  # Common exclusions\n    \n    for root, dirs, files in os.walk(directory):\n        # Filter excluded directories\n        dirs[:] = [d for d in dirs if d not in excluded_dirs]\n        \n        rel_path = os.path.relpath(root, directory)\n        folder_data = {\n            \"path\": rel_path,\n            \"files\": []\n        }\n\n        for file in files:\n            file_path = os.path.join(root, file)\n            try:\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                folder_data[\"files\"].append({\n                    \"name\": file,\n                    \"content\": content,\n                    \"size\": os.path.getsize(file_path)\n                })\n            except UnicodeDecodeError:\n                folder_data[\"files\"].append({\n                    \"name\": file,\n                    \"error\": \"Binary file not decoded\"\n                })\n            except Exception as e:\n                folder_data[\"files\"].append({\n                    \"name\": file,\n                    \"error\": str(e)\n                })\n\n        output.append(folder_data)\n\n    # Save structured output\n    Path(output_dir).mkdir(exist_ok=True)\n    \n    # Single JSON file format\n    output_file = Path(output_dir) / \"codebase_export.json\"\n    with open(output_file, 'w', encoding='utf-8') as f:\n        json.dump(output, f, indent=2)\n    \n    print(\"Export completed!\")  # Debug print\n    return output_dir\n\n# Add this part to actually run the function\nif __name__ == \"__main__\":\n    print(\"Script starting...\")\n    result = export_codebase()\n    print(f\"Files exported to: {result}\")\n    input(\"Press Enter to exit...\")\n\n",
        "size": 2091
      },
      {
        "name": "README.md",
        "content": "",
        "size": 0
      },
      {
        "name": "requirements.txt",
        "content": "# requirements.txt\nfastapi>=0.104.1\nuvicorn>=0.24.0\nhttpx>=0.25.2\naio-pika>=9.3.0\npydantic>=2.5.2\npython-dotenv>=1.0.0\nPyYAML>=6.0.1\nasyncpg>=0.27.0\nsqlalchemy>=2.0.0\nalembic>=1.13.1\npsycopg2-binary>=2.9.9\naiosqlite>=0.19.0\naiohttp>=3.9.0\npython-multipart>=0.0.6\nwatchfiles>=0.19.0\n",
        "size": 298
      },
      {
        "name": "setup.py",
        "content": "from setuptools import setup, find_packages\n\nsetup(\n    name=\"ai_orchestrator\",\n    version=\"0.1\",\n    packages=find_packages(),\n    install_requires=[\n        \"fastapi>=0.104.1\",\n        \"uvicorn>=0.24.0\",\n        \"httpx>=0.25.2\",\n        \"aio-pika>=9.3.0\",\n        \"pydantic>=2.5.2\",\n        \"python-dotenv>=1.0.0\",\n        \"PyYAML>=6.0.1\",\n        \"alembic>=1.7.7\",\n        \"asyncpg>=0.25.0\",\n        \"SQLAlchemy>=1.4.36\",\n        \"psycopg2-binary>=2.9.3\"\n    ]\n)\n",
        "size": 487
      }
    ]
  },
  {
    "path": "config",
    "files": [
      {
        "name": "hierarchy.py",
        "content": "from typing import Dict, List, Optional\nfrom pydantic import BaseModel\n\nclass ServiceLevel(BaseModel):\n    \"\"\"Represents a level in the service hierarchy\"\"\"\n    services: List[str]\n    specialization: str\n\nclass Branch(BaseModel):\n    \"\"\"Represents a branch in the hierarchy\"\"\"\n    coordinator: str\n    levels: List[ServiceLevel]\n    description: str\n\nclass Hierarchy(BaseModel):\n    \"\"\"Complete system hierarchy\"\"\"\n    coordinator: str\n    branches: Dict[str, Branch]\n    description: str\n\n# System hierarchy definition\nSYSTEM_HIERARCHY = Hierarchy(\n    coordinator=\"atlas\",\n    description=\"AI Service Orchestration System\",\n    branches={\n        \"technical\": Branch(\n            coordinator=\"nova\",\n            description=\"Technical analysis branch\",\n            levels=[\n                ServiceLevel(\n                    services=[\"echo\", \"pixel\"],\n                    specialization=\"implementation\"\n                )\n            ]\n        ),\n        \"philosophical\": Branch(\n            coordinator=\"sage\",\n            description=\"Philosophical analysis branch\",\n            levels=[\n                ServiceLevel(\n                    services=[\"quantum\"],\n                    specialization=\"insight\"\n                )\n            ]\n        )\n    }\n)",
        "size": 1305
      },
      {
        "name": "models.py",
        "content": "import os\nfrom typing import Dict, Any, Optional\nfrom pydantic import BaseModel, ConfigDict\n\nclass ModelParameters(BaseModel):\n    model_config = ConfigDict(protected_namespaces=())\n    \n    name: str = \"phi-3\"\n    slot: Optional[int] = None\n    temperature: float = 0.7\n    max_tokens: int = 1024      # Output length\n    context_length: int = 8192  # Plenty of room for input + responses\n    top_p: float = 0.9\n\n    # Service-specific configurations\n    @classmethod\n    def for_service(cls, service_name: str) -> 'ModelParameters':\n        base_config = cls()\n        \n        # Adjust max_tokens based on service role\n        if service_name in ['nova', 'sage', 'atlas']:\n            base_config.max_tokens = 1024  # More tokens for synthesis\n        else:\n            base_config.max_tokens = 512   # Fewer tokens for analysis\n            \n        return base_config\n\nclass ModelConfig(BaseModel):\n    base_url: str\n    models: Dict[str, ModelParameters]\n\n    @classmethod\n    def from_env(cls) -> 'ModelConfig':\n        base_config = {\n            'temperature': 0.7,\n            'max_tokens': 4096,\n            'top_p': 0.9\n        }\n        \n        services = ['atlas', 'nova', 'sage', 'echo', 'pixel', 'quantum']\n        models = {}\n        \n        for service in services:\n            models[service] = ModelParameters(\n                name=\"phi-3\",\n                slot=None if service == 'atlas' else {\n                    'nova': 2, 'sage': 3, 'echo': 4,\n                    'pixel': 5, 'quantum': 6\n                }[service],\n                temperature=float(os.getenv(f'{service.upper()}_TEMP', base_config['temperature'])),\n                max_tokens=int(os.getenv(f'{service.upper()}_MAX_TOKENS', base_config['max_tokens'])),\n                top_p=float(os.getenv(f'{service.upper()}_TOP_P', base_config['top_p']))\n            )\n        \n        return cls(\n            base_url=os.getenv('LMSTUDIO_BASE_URL', 'http://localhost:1234/v1'),\n            models=models\n        )\n\n# Create global model configuration\nMODEL_CONFIG = ModelConfig.from_env()\n\n# Add debug print\nprint(\"MODEL_CONFIG initialized with:\")\nfor service, params in MODEL_CONFIG.models.items():\n    print(f\"{service}: slot={params.slot}, temp={params.temperature}\")",
        "size": 2317
      },
      {
        "name": "processing.py",
        "content": "from typing import List, Dict\nfrom pydantic import BaseModel\n\nclass ProcessComponent(BaseModel):\n    \"\"\"Represents a processing component\"\"\"\n    description: str\n    required_capabilities: List[str]\n\nclass ProcessPattern(BaseModel):\n    \"\"\"Represents a processing pattern\"\"\"\n    steps: List[str]\n    iterations: int = 1\n    wait_for_responses: bool = True\n\n# Basic processing components\nPROCESS_COMPONENTS = {\n    \"analyze\": ProcessComponent(\n        description=\"Examine and understand input\",\n        required_capabilities=[\"comprehension\", \"analysis\"]\n    ),\n    \"reflect\": ProcessComponent(\n        description=\"Consider implications and alternatives\",\n        required_capabilities=[\"reasoning\", \"evaluation\"]\n    ),\n    \"synthesize\": ProcessComponent(\n        description=\"Combine insights into coherent output\",\n        required_capabilities=[\"integration\", \"summarization\"]\n    ),\n    \"delegate\": ProcessComponent(\n        description=\"Send to specialized processing\",\n        required_capabilities=[\"routing\", \"coordination\"]\n    )\n}\n\n# Standard processing patterns\nPROCESS_PATTERNS = {\n    \"standard\": ProcessPattern(\n        steps=[\"analyze\", \"delegate\", \"synthesize\"]\n    ),\n    \"reflective\": ProcessPattern(\n        steps=[\"analyze\", \"reflect\", \"delegate\", \"synthesize\"]\n    ),\n    \"iterative\": ProcessPattern(\n        steps=[\"analyze\", \"delegate\", \"synthesize\", \"reflect\"],\n        iterations=2\n    )\n}",
        "size": 1462
      },
      {
        "name": "services.py",
        "content": "from core.templates import ServiceTemplate\nfrom config.hierarchy import SYSTEM_HIERARCHY\n\n# Create service templates based on hierarchy\ndef create_service_templates():\n    templates = {}\n    \n    # Create coordinator (Atlas)\n    templates[\"atlas\"] = ServiceTemplate.create_coordinator(\n        name=\"atlas\",\n        port=8000\n    )\n\n    # Create branch coordinators and their services\n    for branch_name, branch in SYSTEM_HIERARCHY.branches.items():\n        # Create branch coordinator\n        templates[branch.coordinator] = ServiceTemplate.create_branch(\n            name=branch.coordinator,\n            port=8001 if branch.coordinator == \"nova\" else 8004,  # Example port assignment\n            parent=\"atlas\"\n        )\n        \n        # Create leaf services for each level\n        for level in branch.levels:\n            for service in level.services:\n                templates[service] = ServiceTemplate.create_leaf(\n                    name=service,\n                    port=get_port_for_service(service),  # Helper function needed\n                    parent=branch.coordinator\n                )\n                \n                # Update parent's child queues\n                templates[branch.coordinator].messaging_config.child_queues.append(\n                    f\"{service}_queue\"\n                )\n\n    return templates\n\ndef get_port_for_service(service: str) -> int:\n    \"\"\"Helper function to assign ports to services\"\"\"\n    port_map = {\n        \"echo\": 8002,\n        \"pixel\": 8003,\n        \"quantum\": 8005\n    }\n    return port_map.get(service, 8010)  # Default port if not found\n\n# Create all service templates\nSERVICE_TEMPLATES = create_service_templates()",
        "size": 1719
      },
      {
        "name": "settings.py",
        "content": "import os\nfrom pathlib import Path\nfrom dotenv import load_dotenv\n\n# Load .env file\nenv_path = Path(__file__).parent.parent / '.env'\nload_dotenv(env_path)\n\n# Database settings\nDATABASE_URL = f\"{os.getenv('DB_TYPE', 'postgresql')}+asyncpg://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}\"\n\nDB_POOL_SETTINGS = {\n    'pool_size': int(os.getenv('DB_POOL_SIZE', 5)),\n    'max_overflow': int(os.getenv('DB_MAX_OVERFLOW', 10)),\n    'pool_timeout': int(os.getenv('DB_POOL_TIMEOUT', 30)),\n    'pool_recycle': int(os.getenv('DB_POOL_RECYCLE', 1800))\n}\n\n# Update alembic.ini programmatically\ndef update_alembic_config():\n    from configparser import ConfigParser\n    config = ConfigParser()\n    alembic_ini_path = Path(__file__).parent.parent / 'alembic.ini'\n    config.read(alembic_ini_path)\n    config.set('alembic', 'sqlalchemy.url', DATABASE_URL)\n    with open(alembic_ini_path, 'w') as configfile:\n        config.write(configfile)\n\n# System configuration\nSYSTEM_CONFIG = {\n    'environment': os.getenv('APP_ENV', 'development'),\n    'log_level': os.getenv('LOG_LEVEL', 'INFO'),\n    'log_format': os.getenv('LOG_FORMAT', 'detailed'),\n    'log_to_file': os.getenv('LOG_TO_FILE', 'false').lower() == 'true',\n    'log_file_path': os.getenv('LOG_FILE_PATH', 'logs/ai_orchestrator.log'),\n    'request_timeout': int(os.getenv('REQUEST_TIMEOUT', 240)),\n    'reflection_depth': int(os.getenv('REFLECTION_DEPTH', 2)),\n    'max_retries': int(os.getenv('MAX_RETRIES', 3)),\n    'enable_metrics': os.getenv('ENABLE_METRICS', 'true').lower() == 'true',\n    'metrics_port': int(os.getenv('METRICS_PORT', 9090))\n}\n\n# Service ports\nSERVICE_PORTS = {\n    'atlas': int(os.getenv('ATLAS_PORT', 8000)),\n    'nova': int(os.getenv('NOVA_PORT', 8001)),\n    'echo': int(os.getenv('ECHO_PORT', 8002)),\n    'pixel': int(os.getenv('PIXEL_PORT', 8003)),\n    'sage': int(os.getenv('SAGE_PORT', 8004)),\n    'quantum': int(os.getenv('QUANTUM_PORT', 8005))\n}\n\n# Model configurations\nMODEL_SETTINGS = {\n    'atlas': os.getenv('ATLAS_MODEL'),\n    'nova': os.getenv('NOVA_MODEL'),\n    'sage': os.getenv('SAGE_MODEL'),\n    'echo': os.getenv('ECHO_MODEL'),\n    'pixel': os.getenv('PIXEL_MODEL'),\n    'quantum': os.getenv('QUANTUM_MODEL')\n}\n\n# RabbitMQ settings\nRABBITMQ_CONFIG = {\n    'host': os.getenv('RABBITMQ_HOST', 'localhost'),\n    'port': int(os.getenv('RABBITMQ_PORT', 5672)),\n    'user': os.getenv('RABBITMQ_USER', 'guest'),\n    'password': os.getenv('RABBITMQ_PASSWORD', 'guest'),\n    'vhost': os.getenv('RABBITMQ_VHOST', '/')\n}",
        "size": 2634
      },
      {
        "name": "__init__.py",
        "content": "",
        "size": 0
      }
    ]
  },
  {
    "path": "core",
    "files": [
      {
        "name": ".DS_Store",
        "error": "Binary file not decoded"
      },
      {
        "name": "templates.py",
        "content": "from typing import Dict, List, Optional, Any\nfrom pydantic import BaseModel\nfrom enum import Enum\n\nclass ServiceType(str, Enum):\n    COORDINATOR = \"coordinator\"\n    BRANCH = \"branch\"\n    LEAF = \"leaf\"\n\nclass ServiceCapability(str, Enum):\n    ANALYZE = \"analyze\"\n    DELEGATE = \"delegate\"\n    SYNTHESIZE = \"synthesize\"\n    REFLECT = \"reflect\"\n    COORDINATE = \"coordinate\"\n\nclass ServiceConfig(BaseModel):\n    \"\"\"Base configuration for all services\"\"\"\n    name: str\n    type: ServiceType\n    model_name: str\n    capabilities: List[ServiceCapability]\n    processing_pattern: str\n    port: int\n    parent: Optional[str] = None\n    children: List[str] = []\n    \nclass ModelConfig(BaseModel):\n    \"\"\"LLM configuration\"\"\"\n    name: str\n    endpoint: str\n    context_window: int\n    parameters: Dict[str, Any] = {}\n\nclass MessagingConfig(BaseModel):\n    \"\"\"Messaging configuration\"\"\"\n    queue_name: str\n    parent_queue: Optional[str]\n    child_queues: List[str] = []\n    retry_attempts: int = 3\n    timeout: int = 30\n\nclass ServiceTemplate:\n    \"\"\"Template for service initialization\"\"\"\n    def __init__(\n        self,\n        service_config: ServiceConfig,\n        model_config: ModelConfig,\n        messaging_config: MessagingConfig\n    ):\n        self.service_config = service_config\n        self.model_config = model_config\n        self.messaging_config = messaging_config\n\n    @classmethod\n    def create_coordinator(cls, name: str, port: int) -> 'ServiceTemplate':\n        \"\"\"Create a coordinator service template\"\"\"\n        return cls(\n            service_config=ServiceConfig(\n                name=name,\n                type=ServiceType.COORDINATOR,\n                model_name=\"phi-3\",\n                capabilities=[\n                    ServiceCapability.ANALYZE,\n                    ServiceCapability.DELEGATE,\n                    ServiceCapability.COORDINATE,\n                    ServiceCapability.SYNTHESIZE\n                ],\n                processing_pattern=\"standard\",\n                port=port\n            ),\n            model_config=ModelConfig(\n                name=\"phi-3\",\n                endpoint=\"http://localhost:1234/v1/chat/completions\",\n                context_window=2048\n            ),\n            messaging_config=MessagingConfig(\n                queue_name=f\"{name}_queue\",\n                parent_queue=None,\n                child_queues=[]  # Will be populated based on hierarchy\n            )\n        )\n\n    @classmethod\n    def create_branch(cls, name: str, port: int, parent: str) -> 'ServiceTemplate':\n        \"\"\"Create a branch service template\"\"\"\n        return cls(\n            service_config=ServiceConfig(\n                name=name,\n                type=ServiceType.BRANCH,\n                model_name=\"phi-3\",\n                capabilities=[\n                    ServiceCapability.ANALYZE,\n                    ServiceCapability.DELEGATE,\n                    ServiceCapability.SYNTHESIZE\n                ],\n                processing_pattern=\"standard\",\n                port=port,\n                parent=parent\n            ),\n            model_config=ModelConfig(\n                name=\"phi-3\",\n                endpoint=\"http://localhost:1234/v1/chat/completions\",\n                context_window=2048\n            ),\n            messaging_config=MessagingConfig(\n                queue_name=f\"{name}_queue\",\n                parent_queue=f\"{parent}_queue\",\n                child_queues=[]  # Will be populated based on hierarchy\n            )\n        )\n\n    @classmethod\n    def create_leaf(cls, name: str, port: int, parent: str) -> 'ServiceTemplate':\n        \"\"\"Create a leaf service template\"\"\"\n        return cls(\n            service_config=ServiceConfig(\n                name=name,\n                type=ServiceType.LEAF,\n                model_name=\"phi-3\",\n                capabilities=[\n                    ServiceCapability.ANALYZE,\n                    ServiceCapability.SYNTHESIZE\n                ],\n                processing_pattern=\"standard\",\n                port=port,\n                parent=parent\n            ),\n            model_config=ModelConfig(\n                name=\"phi-3\",\n                endpoint=\"http://localhost:1234/v1/chat/completions\",\n                context_window=2048\n            ),\n            messaging_config=MessagingConfig(\n                queue_name=f\"{name}_queue\",\n                parent_queue=f\"{parent}_queue\",\n                child_queues=[]\n            )\n        )",
        "size": 4576
      },
      {
        "name": "validation.py",
        "content": "# New file: core/validation.py\nfrom typing import Dict, Any\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass MessageValidator:\n    @staticmethod\n    def validate_message_content(content: str, max_length: int = 8192) -> bool:\n        \"\"\"Validate message content\"\"\"\n        if not content:\n            logger.error(\"Empty message content\")\n            return False\n        \n        if len(content) > max_length:\n            logger.error(f\"Message content exceeds max length: {len(content)} > {max_length}\")\n            return False\n            \n        return True\n\n    @staticmethod\n    def validate_message_structure(message: Dict[str, Any]) -> bool:\n        \"\"\"Validate message structure\"\"\"\n        required_fields = ['type', 'content', 'correlation_id', 'source', 'destination']\n        \n        for field in required_fields:\n            if field not in message:\n                logger.error(f\"Missing required field: {field}\")\n                return False\n                \n        return True",
        "size": 1036
      },
      {
        "name": "__init__.py",
        "content": "",
        "size": 0
      }
    ]
  },
  {
    "path": "core\\logging",
    "files": [
      {
        "name": "system_logger.py",
        "content": "# core/logging/system_logger.py\nimport asyncio\nfrom datetime import datetime\nfrom typing import Dict, Any, Optional, List\nfrom sqlalchemy import select\nfrom sqlalchemy.exc import IntegrityError\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom database.models import Conversation, Message, ProcessingMetrics, ConversationStatus\nfrom core.messaging.types import MessageType\nfrom database.connection import get_db_session\n\nclass SystemLogger:\n    @staticmethod\n    async def start_conversation(query: str) -> int:\n        \"\"\"Start a new conversation and return its ID\"\"\"\n        async with get_db_session() as session:\n            conversation = Conversation(\n                started_at=datetime.utcnow(),\n                initial_query=query,\n                status='active'\n            )\n            session.add(conversation)\n            await session.flush()\n            return conversation.id\n\n    @staticmethod\n    async def end_conversation(conversation_id: int, status: str = 'completed'):\n        \"\"\"Mark a conversation as complete\"\"\"\n        async with get_db_session() as session:\n            conversation = await session.get(Conversation, conversation_id)\n            if conversation:\n                conversation.ended_at = datetime.utcnow()\n                conversation.status = status\n                await session.commit()\n\n    @staticmethod\n    async def log_message(\n        conversation_id: int,\n        message_type: str,\n        source: str,\n        destination: str,\n        content: str,\n        correlation_id: str,\n        context: Dict[str, Any] = None\n    ) -> int:\n        \"\"\"Log a message and return its ID\"\"\"\n        async with get_db_session() as session:\n            try:\n                message = Message(\n                    conversation_id=conversation_id,\n                    timestamp=datetime.utcnow(),\n                    message_type=message_type,\n                    source=source,\n                    destination=destination,\n                    content=content,\n                    correlation_id=correlation_id,\n                    context=context or {},\n                    processing_details={}\n                )\n                session.add(message)\n                await session.flush()\n                await session.commit()\n                return message.id\n            except IntegrityError:\n                await session.rollback()\n                # Get existing message\n                existing = await session.execute(\n                    select(Message).where(\n                        Message.conversation_id == conversation_id,\n                        Message.message_type == message_type,\n                        Message.source == source,\n                        Message.destination == destination,\n                        Message.correlation_id == correlation_id\n                    )\n                )\n                existing_message = existing.scalar_one_or_none()\n                if existing_message:\n                    return existing_message.id\n                raise\n\n    @staticmethod\n    async def log_processing_metrics(\n        message_id: int,\n        service: str,\n        operation_type: str,\n        tokens_used: int,\n        processing_time: float,\n        model_parameters: Dict[str, Any] = None\n    ):\n        \"\"\"Log processing metrics for a message\"\"\"\n        async with get_db_session() as session:\n            metrics = ProcessingMetrics(\n                message_id=message_id,\n                timestamp=datetime.utcnow(),\n                service=service,\n                operation_type=operation_type,\n                tokens_used=tokens_used,\n                processing_time=processing_time,\n                model_parameters=model_parameters or {}\n            )\n            session.add(metrics)\n            await session.commit()\n\n    @staticmethod\n    async def get_conversation_messages(conversation_id: int) -> List[Dict[str, Any]]:\n        \"\"\"Get all messages for a conversation\"\"\"\n        async with get_db_session() as session:\n            result = await session.execute(\n                select(Message)\n                .where(Message.conversation_id == conversation_id)\n                .order_by(Message.timestamp)\n            )\n            messages = result.scalars().all()\n            return [\n                {\n                    \"id\": msg.id,\n                    \"timestamp\": msg.timestamp,\n                    \"type\": msg.message_type,\n                    \"source\": msg.source,\n                    \"destination\": msg.destination,\n                    \"content\": msg.content,\n                    \"correlation_id\": msg.correlation_id,\n                    \"properties\": msg.properties\n                }\n                for msg in messages\n            ]",
        "size": 4864
      }
    ]
  },
  {
    "path": "core\\messaging",
    "files": [
      {
        "name": "rabbit.py",
        "content": "import json\nimport asyncio\nimport aio_pika\nfrom typing import Dict, Any, Callable, Optional\nfrom core.utils.logging import setup_logger\n\nlogger = setup_logger(\"rabbitmq\")\n\nclass RabbitMQHandler:\n    def __init__(self, queue_name: str, parent_queue: Optional[str] = None):\n        self.queue_name = queue_name\n        self.parent_queue = parent_queue\n        self.connection = None\n        self.channel = None\n        self.queue = None\n        self.processed_messages = set()\n        self.processing_lock = asyncio.Lock()\n\n    async def connect(self) -> None:\n        \"\"\"Establish connection to RabbitMQ\"\"\"\n        try:\n            # Create robust connection\n            self.connection = await aio_pika.connect_robust(\n                host=\"localhost\",\n                port=5672,\n                login=\"guest\",\n                password=\"guest\"\n            )\n            \n            # Create channel\n            self.channel = await self.connection.channel()\n            await self.channel.set_qos(prefetch_count=1)\n            \n            # Declare queue with basic configuration\n            self.queue = await self.channel.declare_queue(\n                self.queue_name,\n                durable=True,\n                auto_delete=False\n            )\n            \n            logger.info(f\"Connected to RabbitMQ and declared queue: {self.queue_name}\")\n            \n        except Exception as e:\n            logger.error(f\"Error connecting to RabbitMQ: {e}\")\n            raise\n\n    async def start_consuming(self, callback: Callable) -> None:\n        \"\"\"Start consuming messages from the queue\"\"\"\n        try:\n            async def _process_message(message: aio_pika.IncomingMessage) -> None:\n                async with message.process():\n                    message_id = message.message_id\n                    if message_id in self.processed_messages:\n                        logger.warning(f\"Duplicate message detected: {message_id}\")\n                        return\n                        \n                    async with self.processing_lock:\n                        if message_id not in self.processed_messages:\n                            body = json.loads(message.body.decode())\n                            await callback(body)\n                            self.processed_messages.add(message_id)\n                            \n                            # Cleanup processed messages set if it gets too large\n                            if len(self.processed_messages) > 10000:\n                                self.processed_messages.clear()\n\n            await self.queue.consume(_process_message)\n            logger.info(f\"Started consuming from queue: {self.queue_name}\")\n            \n        except Exception as e:\n            logger.error(f\"Error setting up consumer: {e}\")\n            raise\n\n    async def publish(self, queue_name: str, message: Dict[str, Any]) -> None:\n        \"\"\"Publish message to queue\"\"\"\n        try:\n            if not self.channel:\n                await self.connect()\n            \n            # Create unique message identifier\n            message_id = f\"{message.get('correlation_id')}:{message.get('type')}:{message.get('source')}:{message.get('destination')}\"\n            \n            message_body = json.dumps(message).encode()\n            rabbitmq_message = aio_pika.Message(\n                message_body,\n                message_id=message_id,  # Use consistent message ID\n                delivery_mode=aio_pika.DeliveryMode.PERSISTENT\n            )\n            \n            await self.channel.declare_queue(\n                queue_name,\n                durable=True,\n                auto_delete=False\n            )\n            \n            await self.channel.default_exchange.publish(\n                rabbitmq_message,\n                routing_key=queue_name\n            )\n            \n            logger.debug(f\"Published message {message_id} to queue: {queue_name}\")\n            \n        except Exception as e:\n            logger.error(f\"Error publishing message: {e}\")\n            raise\n\n    async def cleanup(self) -> None:\n        \"\"\"Cleanup resources\"\"\"\n        try:\n            if self.channel:\n                await self.channel.close()\n            if self.connection:\n                await self.connection.close()\n            logger.info(\"RabbitMQ connections closed\")\n        except Exception as e:\n            logger.error(f\"Error during cleanup: {e}\")",
        "size": 4518
      },
      {
        "name": "types.py",
        "content": "from enum import Enum\nfrom typing import Optional, Dict, Any\nfrom pydantic import BaseModel\n\nclass MessageType(str, Enum):\n    # Basic message flow\n    DELEGATION = \"delegation\"    # Parent delegating to child\n    RESPONSE = \"response\"        # Child responding to parent\n    QUERY = \"query\"             # Initial user query\n    \n    # Cognitive operations\n    REFLECTION = \"reflection\"    # Think about previous thinking\n    ITERATION = \"iteration\"      # Multiple exchanges\n    SYNTHESIS = \"synthesis\"      # Combine multiple thoughts\n    CRITIQUE = \"critique\"        # Critical analysis of previous thought\n    EXPANSION = \"expansion\"      # Expand on a specific aspect\n    \n    # Meta operations\n    STATUS = \"status\"           # Process status updates\n    ERROR = \"error\"             # Error messages\n    CONTROL = \"control\"         # Flow control messages\n\nclass Message(BaseModel):\n    type: MessageType\n    content: str\n    correlation_id: str\n    source: str\n    destination: str\n    iteration: int = 1\n    conversation_id: Optional[int] = None\n    context: Dict[str, Any] = {}\n    \n    # For reflection/iteration chains\n    previous_response: Optional[str] = None\n    reflection_depth: int = 0\n    thinking_pattern: Optional[str] = None  # e.g., \"analyze->reflect->synthesize\"",
        "size": 1321
      },
      {
        "name": "__init__.py",
        "content": "",
        "size": 0
      }
    ]
  },
  {
    "path": "core\\services",
    "files": [
      {
        "name": "base.py",
        "content": "# base.py\nimport asyncio\nfrom typing import Dict, Any, Optional\nimport httpx\nfrom fastapi import FastAPI, HTTPException\nfrom core.utils.logging import setup_logger\nfrom core.messaging.rabbit import RabbitMQHandler\nfrom core.templates import ServiceTemplate, ServiceType\nfrom config.models import MODEL_CONFIG\n    \nlogger = setup_logger(\"service\")\n    \nclass BaseService:\n    def __init__(self, template: ServiceTemplate):\n        self.template = template\n        self.app = FastAPI()\n        self.messaging = RabbitMQHandler(\n            template.messaging_config.queue_name,\n            template.messaging_config.parent_queue\n        )\n        self._setup_routes()\n    \n    def _setup_routes(self):\n        \"\"\"Setup basic HTTP endpoints\"\"\"\n        @self.app.get(\"/health\")\n        async def health_check():\n            return {\n                \"status\": \"healthy\",\n                \"service\": self.template.service_config.name,\n                \"type\": self.template.service_config.type\n            }\n    \n    async def start(self):\n        \"\"\"Initialize and start the service\"\"\"\n        logger.info(f\"Starting {self.template.service_config.name} service...\")\n            \n        # Connect to RabbitMQ\n        await self.messaging.connect()\n            \n        # Start message consumption\n        await self.messaging.start_consuming(self.process_message)\n            \n        # Start FastAPI server\n        import uvicorn\n        config = uvicorn.Config(\n            self.app,\n                host=\"localhost\",\n                port=self.template.service_config.port,\n                log_level=\"info\"\n            )\n        server = uvicorn.Server(config)\n        await server.serve()\n    \n    async def process_message(self, message: Dict[str, Any]):\n        \"\"\"Process incoming messages - to be implemented by specific services\"\"\"\n        raise NotImplementedError\n    \n    async def query_model(self, prompt: str) -> Dict[str, Any]:\n        \"\"\"Query the LLM model with ordered timing strategy\"\"\"\n        max_retries = 3\n        base_delay = 5  # seconds between retries\n        \n        # Service processing order and initial delays\n        service_delays = {\n            'atlas': 0,      # Starts immediately\n            'nova': 10,      # Waits 10 seconds after getting Atlas's message\n            'sage': 15,      # Waits 15 seconds after getting Atlas's message\n            'echo': 20,      # Waits 20 seconds after getting Nova's message\n            'pixel': 25,     # Waits 25 seconds after getting Nova's message\n            'quantum': 30,   # Waits 30 seconds after getting Sage's message\n        }\n        \n        service_name = self.template.service_config.name\n        logger.info(f\"Service {service_name} preparing to process\")\n        \n        # Initial service-specific delay\n        initial_delay = service_delays.get(service_name, 0)\n        if initial_delay > 0:\n            logger.info(f\"Service {service_name} waiting {initial_delay} seconds before starting\")\n            await asyncio.sleep(initial_delay)\n        \n        for attempt in range(max_retries):\n            try:\n                if attempt > 0:\n                    retry_delay = base_delay * (2 ** (attempt - 1))  # Exponential backoff\n                    logger.info(f\"Service {service_name} retry {attempt + 1}/{max_retries} after {retry_delay} seconds\")\n                    await asyncio.sleep(retry_delay)\n                \n                logger.info(f\"Service {service_name} attempting query (attempt {attempt + 1}/{max_retries})\")\n                \n                # Get model parameters\n                if service_name not in MODEL_CONFIG.models:\n                    raise ValueError(f\"No model configuration found for service: {service_name}\")\n                \n                model_params = MODEL_CONFIG.models[service_name]\n                \n                # Base model name\n                base_name = \"lmstudio-community/Phi-3.1-mini-128k-instruct-GGUF/Phi-3.1-mini-128k-instruct-Q4_K_M.gguf\"\n                \n                # Special handling for Atlas (no slot number)\n                model_name = base_name if service_name == 'atlas' else f\"{base_name}:{model_params.slot}\"\n                \n                logger.info(f\"Service {service_name} using model: {model_name}\")\n                \n                async with httpx.AsyncClient(timeout=30.0) as client:\n                    request_data = {\n                        \"model\": model_name,\n                        \"messages\": [\n                            {\"role\": \"user\", \"content\": prompt}\n                        ],\n                        \"temperature\": model_params.temperature,\n                        \"max_tokens\": model_params.max_tokens,\n                        \"top_p\": model_params.top_p,\n                        \"stream\": False\n                    }\n                    \n                    response = await client.post(\n                        f\"{MODEL_CONFIG.base_url}/chat/completions\",\n                        json=request_data,\n                        headers={\"Content-Type\": \"application/json\"}\n                    )\n                    \n                    if response.status_code == 200:\n                        result = response.json()\n                        logger.info(f\"Service {service_name} query successful\")\n                        return result\n                    else:\n                        error_msg = f\"Service {service_name} query failed with status {response.status_code}: {response.text}\"\n                        logger.error(error_msg)\n                        raise HTTPException(status_code=response.status_code, detail=error_msg)\n                        \n            except Exception as e:\n                logger.error(f\"Service {service_name} error on attempt {attempt + 1}: {str(e)}\")\n                if attempt == max_retries - 1:\n                    raise Exception(f\"Service {service_name} failed after {max_retries} attempts: {str(e)}\")\n    \n    async def cleanup(self):\n        \"\"\"Cleanup resources\"\"\"\n        await self.messaging.cleanup()",
        "size": 6180
      },
      {
        "name": "__init__.py",
        "content": "",
        "size": 0
      }
    ]
  },
  {
    "path": "core\\utils",
    "files": [
      {
        "name": "logging.py",
        "content": "import logging\nimport sys\nfrom typing import Optional\n\ndef setup_logger(name: str, level: Optional[int] = None) -> logging.Logger:\n    \"\"\"Configure and return a logger instance\"\"\"\n    logger = logging.getLogger(name)\n    \n    if level is None:\n        level = logging.INFO\n    \n    logger.setLevel(level)\n    \n    if not logger.handlers:\n        handler = logging.StreamHandler(sys.stdout)\n        handler.setLevel(level)\n        \n        formatter = logging.Formatter(\n            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n        )\n        handler.setFormatter(formatter)\n        \n        logger.addHandler(handler)\n    \n    return logger",
        "size": 677
      },
      {
        "name": "__init__.py",
        "content": "",
        "size": 0
      }
    ]
  },
  {
    "path": "database",
    "files": [
      {
        "name": ".DS_Store",
        "error": "Binary file not decoded"
      },
      {
        "name": "connection.py",
        "content": "from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\nfrom sqlalchemy.orm import sessionmaker\nfrom contextlib import asynccontextmanager\nfrom config.settings import DATABASE_URL, DB_POOL_SETTINGS\n\n# Create engine with settings from config\nengine = create_async_engine(\n    DATABASE_URL,\n    **DB_POOL_SETTINGS,\n    echo=True\n)\n\n# Create session factory\nAsyncSessionLocal = sessionmaker(\n    engine,\n    class_=AsyncSession,\n    expire_on_commit=False\n)\n\n@asynccontextmanager\nasync def get_db_session():\n    \"\"\"Provide a transactional scope around a series of operations.\"\"\"\n    async with AsyncSessionLocal() as session:\n        try:\n            yield session\n            await session.commit()\n        except Exception as e:\n            await session.rollback()\n            raise",
        "size": 823
      },
      {
        "name": "logger.py",
        "content": "from typing import Dict, Any\nfrom datetime import datetime\nfrom .connection import get_db_session\nfrom .models import ConversationLog, MessageLog, ProcessingMetrics\n\nclass SystemLogger:\n    @staticmethod\n    async def start_conversation(query: str) -> int:\n        async with get_db_session() as session:\n            conversation = ConversationLog(\n                initial_query=query,\n                status=\"active\"\n            )\n            session.add(conversation)\n            await session.flush()\n            return conversation.id\n\n    @staticmethod\n    async def log_message(\n        conversation_id: int,\n        message_type: str,\n        source: str,\n        destination: str,\n        content: str,\n        correlation_id: str,\n        context: Dict[str, Any] = None,\n        parent_message_id: Optional[int] = None\n    ) -> int:\n        async with get_db_session() as session:\n            message = MessageLog(\n                conversation_id=conversation_id,\n                message_type=message_type,\n                source=source,\n                destination=destination,\n                content=content,\n                correlation_id=correlation_id,\n                context=context or {},\n                parent_message_id=parent_message_id\n            )\n            session.add(message)\n            await session.flush()\n            return message.id\n\n    @staticmethod\n    async def log_metrics(\n        message_id: int,\n        service: str,\n        operation_type: str,\n        tokens_used: int,\n        processing_time: float,\n        model_parameters: Dict[str, Any] = None\n    ):\n        async with get_db_session() as session:\n            metrics = ProcessingMetrics(\n                message_id=message_id,\n                service=service,\n                operation_type=operation_type,\n                tokens_used=tokens_used,\n                processing_time=processing_time,\n                model_parameters=model_parameters or {}\n            )\n            session.add(metrics)",
        "size": 2066
      },
      {
        "name": "models.py",
        "content": "from datetime import datetime\nfrom typing import Optional\nfrom enum import Enum\nfrom sqlalchemy import (\n    Column, \n    Integer, \n    String, \n    Text, \n    Float,\n    DateTime, \n    ForeignKey, \n    JSON, \n    UniqueConstraint\n)\nfrom sqlalchemy.orm import relationship, declarative_base\n\nBase = declarative_base()\n\nclass ConversationStatus(str, Enum):\n    ACTIVE = \"active\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n    TIMEOUT = \"timeout\"\n\nclass Conversation(Base):\n    __tablename__ = \"conversation_logs\"\n    \n    id = Column(Integer, primary_key=True, index=True)\n    started_at = Column(DateTime, default=datetime.utcnow, nullable=False)\n    ended_at = Column(DateTime, nullable=True)\n    initial_query = Column(String)\n    status = Column(String)\n    \n    # Define the relationship to Message\n    messages = relationship(\"Message\", back_populates=\"conversation\")\n\nclass Message(Base):\n    __tablename__ = \"message_logs\"\n    \n    id = Column(Integer, primary_key=True, index=True)\n    conversation_id = Column(Integer, ForeignKey(\"conversation_logs.id\"))\n    timestamp = Column(DateTime, default=datetime.utcnow, nullable=False)\n    message_type = Column(String)\n    source = Column(String)\n    destination = Column(String)\n    content = Column(String)\n    correlation_id = Column(String)\n    context = Column(JSON, nullable=True)\n    processing_details = Column(JSON, nullable=True)\n    parent_message_id = Column(Integer, ForeignKey(\"message_logs.id\"), nullable=True)\n    \n    # Define the relationship to Conversation\n    conversation = relationship(\"Conversation\", back_populates=\"messages\")\n    \n    # Add unique constraint\n    __table_args__ = (\n        UniqueConstraint(\n            'conversation_id', \n            'message_type', \n            'source', \n            'destination', \n            'correlation_id',\n            name='uq_message_identifier'\n        ),\n    )\n\nclass ProcessingMetrics(Base):\n    __tablename__ = \"processing_metrics\"\n    \n    id = Column(Integer, primary_key=True, index=True)\n    message_id = Column(Integer, ForeignKey(\"message_logs.id\"))\n    timestamp = Column(DateTime, default=datetime.utcnow, nullable=False)\n    service = Column(String)\n    operation_type = Column(String)\n    tokens_used = Column(Integer)\n    processing_time = Column(Float)\n    model_parameters = Column(JSON, nullable=True)\n    \n    # Define the relationship to Message\n    message = relationship(\"Message\")\n\ndef get_all_models():\n    \"\"\"Return all model classes for verification\"\"\"\n    return [Conversation, Message, ProcessingMetrics]",
        "size": 2646
      },
      {
        "name": "requirements-db.txt",
        "content": "alembic>=1.7.7\npsycopg2-binary>=2.9.3\nSQLAlchemy>=1.4.36\nasyncpg>=0.25.0",
        "size": 75
      },
      {
        "name": "viewer.py",
        "content": "import asyncio\nfrom datetime import datetime, timedelta\nfrom rich.console import Console\nfrom rich.table import Table\nfrom .connection import get_db_session\nfrom .models import ConversationLog, MessageLog, ProcessingMetrics\n\nconsole = Console()\n\nasync def view_recent_conversations():\n    async with get_db_session() as session:\n        recent = await session.execute(\n            select(ConversationLog)\n            .order_by(ConversationLog.started_at.desc())\n            .limit(10)\n        )\n        \n        table = Table(title=\"Recent Conversations\")\n        table.add_column(\"ID\")\n        table.add_column(\"Started\")\n        table.add_column(\"Status\")\n        table.add_column(\"Query\")\n        \n        for conv in recent.scalars():\n            table.add_row(\n                str(conv.id),\n                conv.started_at.strftime(\"%Y-%m-%d %H:%M:%S\"),\n                conv.status,\n                conv.initial_query[:50] + \"...\"\n            )\n        \n        console.print(table)\n\nasync def view_conversation_flow(conversation_id: int):\n    async with get_db_session() as session:\n        messages = await session.execute(\n            select(MessageLog)\n            .where(MessageLog.conversation_id == conversation_id)\n            .order_by(MessageLog.timestamp)\n        )\n        \n        table = Table(title=f\"Conversation Flow - ID: {conversation_id}\")\n        table.add_column(\"Time\")\n        table.add_column(\"Type\")\n        table.add_column(\"From\")\n        table.add_column(\"To\")\n        table.add_column(\"Content Preview\")\n        \n        for msg in messages.scalars():\n            table.add_row(\n                msg.timestamp.strftime(\"%H:%M:%S\"),\n                msg.message_type,\n                msg.source,\n                msg.destination,\n                msg.content[:50] + \"...\"\n            )\n        \n        console.print(table)",
        "size": 1910
      }
    ]
  },
  {
    "path": "migrations",
    "files": [
      {
        "name": ".DS_Store",
        "error": "Binary file not decoded"
      },
      {
        "name": "env.py",
        "content": "# migrations/env.py\nimport asyncio\nfrom logging.config import fileConfig\nfrom sqlalchemy import pool, Float, DateTime, String, Integer, JSON, ForeignKey\nfrom sqlalchemy.engine import Connection\nfrom sqlalchemy.ext.asyncio import async_engine_from_config\nfrom alembic import context\n\n# Import your models\nfrom database.models import Base\n\n# this is the Alembic Config object\nconfig = context.config\n\n# Interpret the config file for Python logging\nif config.config_file_name is not None:\n    fileConfig(config.config_file_name)\n\n# Set target metadata\ntarget_metadata = Base.metadata\n\ndef run_migrations_offline() -> None:\n    \"\"\"Run migrations in 'offline' mode.\"\"\"\n    url = config.get_main_option(\"sqlalchemy.url\")\n    context.configure(\n        url=url,\n        target_metadata=target_metadata,\n        literal_binds=True,\n        dialect_opts={\"paramstyle\": \"named\"},\n    )\n    \n    with context.begin_transaction():\n        context.run_migrations()\n\ndef do_run_migrations(connection: Connection) -> None:\n    context.configure(connection=connection, target_metadata=target_metadata)\n    \n    with context.begin_transaction():\n        context.run_migrations()\n\nasync def run_migrations_online() -> None:\n    \"\"\"Run migrations in 'online' mode.\"\"\"\n    connectable = async_engine_from_config(\n        config.get_section(config.config_ini_section),\n        prefix=\"sqlalchemy.\",\n        poolclass=pool.NullPool,\n    )\n    \n    async with connectable.connect() as connection:\n        await connection.run_sync(do_run_migrations)\n    \n    await connectable.dispose()\n\ndef run_async_migrations():\n    \"\"\"Synchronous wrapper for running async migrations\"\"\"\n    asyncio.run(run_migrations_online())\n\nif context.is_offline_mode():\n    run_migrations_offline()\nelse:\n    run_async_migrations()",
        "size": 1845
      }
    ]
  },
  {
    "path": "migrations\\versions",
    "files": [
      {
        "name": ".DS_Store",
        "error": "Binary file not decoded"
      },
      {
        "name": "001_initial.py",
        "content": "\"\"\"initial\n\nRevision ID: 001_initial\nRevises: \nCreate Date: 2024-01-10 20:00:00.000000\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = '001_initial'\ndown_revision = None\nbranch_labels = None\ndepends_on = None\n\ndef upgrade() -> None:\n    # Create conversation_logs table\n    op.create_table(\n        'conversation_logs',\n        sa.Column('id', sa.Integer(), nullable=False),\n        sa.Column('started_at', sa.DateTime(), nullable=False),\n        sa.Column('ended_at', sa.DateTime(), nullable=True),\n        sa.Column('initial_query', sa.String(), nullable=False),\n        sa.Column('status', sa.String(), nullable=False),\n        sa.PrimaryKeyConstraint('id')\n    )\n\n    # Create message_logs table\n    op.create_table(\n        'message_logs',\n        sa.Column('id', sa.Integer(), nullable=False),\n        sa.Column('conversation_id', sa.Integer(), nullable=False),\n        sa.Column('timestamp', sa.DateTime(), nullable=False),\n        sa.Column('message_type', sa.String(), nullable=False),  # Changed to String\n        sa.Column('source', sa.String(), nullable=False),\n        sa.Column('destination', sa.String(), nullable=False),\n        sa.Column('content', sa.String(), nullable=False),\n        sa.Column('correlation_id', sa.String(), nullable=False),\n        sa.Column('context', postgresql.JSON(), nullable=True),\n        sa.Column('processing_details', postgresql.JSON(), nullable=True),\n        sa.Column('parent_message_id', sa.Integer(), nullable=True),\n        sa.ForeignKeyConstraint(['conversation_id'], ['conversation_logs.id'], ),\n        sa.ForeignKeyConstraint(['parent_message_id'], ['message_logs.id'], ),\n        sa.PrimaryKeyConstraint('id')\n    )\n\n    # Create processing_metrics table\n    op.create_table(\n        'processing_metrics',\n        sa.Column('id', sa.Integer(), nullable=False),\n        sa.Column('message_id', sa.Integer(), nullable=False),\n        sa.Column('timestamp', sa.DateTime(), nullable=False),\n        sa.Column('service', sa.String(), nullable=False),\n        sa.Column('operation_type', sa.String(), nullable=False),\n        sa.Column('tokens_used', sa.Integer(), nullable=False),\n        sa.Column('processing_time', sa.Float(), nullable=False),\n        sa.Column('model_parameters', postgresql.JSON(), nullable=True),\n        sa.ForeignKeyConstraint(['message_id'], ['message_logs.id'], ),\n        sa.PrimaryKeyConstraint('id')\n    )\n\n    # Add indexes\n    op.create_index('ix_conversation_logs_started_at', 'conversation_logs', ['started_at'])\n    op.create_index('ix_message_logs_timestamp', 'message_logs', ['timestamp'])\n    op.create_index('ix_message_logs_correlation_id', 'message_logs', ['correlation_id'])\n    op.create_index('ix_processing_metrics_timestamp', 'processing_metrics', ['timestamp'])\n\ndef downgrade() -> None:\n    # Drop indexes\n    op.drop_index('ix_processing_metrics_timestamp')\n    op.drop_index('ix_message_logs_correlation_id')\n    op.drop_index('ix_message_logs_timestamp')\n    op.drop_index('ix_conversation_logs_started_at')\n    \n    # Drop tables\n    op.drop_table('processing_metrics')\n    op.drop_table('message_logs')\n    op.drop_table('conversation_logs')",
        "size": 3327
      },
      {
        "name": "002_test.py",
        "content": "\"\"\"test migration\n\nRevision ID: 002_test\nRevises: 001_initial\nCreate Date: 2024-01-10 21:00:00.000000\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n# revision identifiers, used by Alembic.\nrevision = '002_test'\ndown_revision = '001_initial'\nbranch_labels = None\ndepends_on = None\n\ndef upgrade() -> None:\n    # Create a simple test table\n    op.create_table(\n        'test_table',\n        sa.Column('id', sa.Integer(), nullable=False),\n        sa.Column('name', sa.String(), nullable=False),\n        sa.Column('value', sa.Float(), nullable=True),\n        sa.PrimaryKeyConstraint('id')\n    )\n\ndef downgrade() -> None:\n    op.drop_table('test_table')",
        "size": 681
      }
    ]
  },
  {
    "path": "scripts",
    "files": [
      {
        "name": ".DS_Store",
        "error": "Binary file not decoded"
      },
      {
        "name": "8",
        "content": "",
        "size": 0
      },
      {
        "name": "check_dependencies.py",
        "content": "# scripts/check_dependencies.py\nimport sys\nfrom pathlib import Path\n\ndef check_dependencies():\n    print(\"Checking Python version:\")\n    print(f\"Python: {sys.version}\")\n    \n    print(\"\\nChecking required packages:\")\n    packages = [\n        'sqlalchemy',\n        'alembic',\n        'psycopg2',\n        'asyncpg',\n        'pydantic'\n    ]\n    \n    for package in packages:\n        try:\n            module = __import__(package)\n            print(f\"{package}: {module.__version__ if hasattr(module, '__version__') else 'installed'}\")\n        except ImportError:\n            print(f\"{package}: NOT INSTALLED\")\n    \n    print(\"\\nChecking SQLAlchemy types:\")\n    try:\n        from sqlalchemy import Float, Integer, String, DateTime, JSON\n        print(\"SQLAlchemy basic types: Available\")\n    except ImportError as e:\n        print(f\"SQLAlchemy types error: {e}\")\n    \n    print(\"\\nChecking Alembic components:\")\n    try:\n        from alembic import op\n        from alembic.config import Config\n        print(\"Alembic components: Available\")\n    except ImportError as e:\n        print(f\"Alembic components error: {e}\")\n    \n    print(\"\\nChecking project structure:\")\n    root = Path(__file__).parent.parent\n    critical_files = [\n        'migrations/env.py',\n        'migrations/versions/001_initial.py',\n        'alembic.ini'\n    ]\n    \n    for file_path in critical_files:\n        full_path = root / file_path\n        if full_path.exists():\n            with open(full_path, 'r') as f:\n                first_line = f.readline().strip()\n            print(f\"{file_path}: EXISTS (First line: {first_line})\")\n        else:\n            print(f\"{file_path}: MISSING\")\n\nif __name__ == \"__main__\":\n    check_dependencies()",
        "size": 1767
      },
      {
        "name": "check_setup.py",
        "content": "# scripts/check_setup.py\nimport os\nfrom pathlib import Path\n\ndef check_project_structure():\n    root_dir = Path(__file__).parent.parent\n    \n    required_structure = {\n        \"migrations\": {\n            \"versions\": [\"001_initial.py\"],\n            \"files\": [\"env.py\"]\n        },\n        \"config\": {\"files\": [\"settings.py\"]},\n        \"database\": {\"files\": [\"models.py\", \"connection.py\"]},\n        \"scripts\": {\"files\": [\"setup_fresh.py\", \"reset_db.py\", \"run_migrations.py\"]},\n        \"root_files\": [\"alembic.ini\"]\n    }\n    \n    print(\"Checking project structure...\")\n    \n    for dir_name, contents in required_structure.items():\n        if dir_name == \"root_files\":\n            # Check root-level files\n            for file_name in contents:\n                file_path = root_dir / file_name\n                print(f\"\\nChecking root file: {file_name}\")\n                print(f\"Exists: {file_path.exists()}\")\n                if file_path.exists():\n                    print(f\"Size: {file_path.stat().st_size} bytes\")\n        else:\n            # Check directories and their contents\n            dir_path = root_dir / dir_name\n            print(f\"\\nChecking directory: {dir_name}\")\n            print(f\"Exists: {dir_path.exists()}\")\n            \n            if dir_path.exists():\n                # Check subdirectories\n                for subdir_name, files in contents.items():\n                    if subdir_name == \"files\":\n                        # Check files in current directory\n                        for file_name in files:\n                            file_path = dir_path / file_name\n                            print(f\"  File {file_name}: {'exists' if file_path.exists() else 'MISSING'}\")\n                    else:\n                        # Check subdirectory\n                        subdir_path = dir_path / subdir_name\n                        print(f\"\\n  Checking subdirectory: {subdir_name}\")\n                        print(f\"  Exists: {subdir_path.exists()}\")\n                        if subdir_path.exists():\n                            for file_name in files:\n                                file_path = subdir_path / file_name\n                                print(f\"    File {file_name}: {'exists' if file_path.exists() else 'MISSING'}\")\n\nif __name__ == \"__main__\":\n    check_project_structure()",
        "size": 2360
      },
      {
        "name": "cleanup_db.py",
        "content": "# scripts/cleanup_tables.py\nimport sys\nimport os\nfrom pathlib import Path\n\n# Add project root to Python path\nroot_dir = Path(__file__).parent.parent\nsys.path.append(str(root_dir))\n\nimport asyncio\nfrom database.connection import engine\nfrom database.models import Base\n\nasync def cleanup_tables():\n    print(\"Dropping all tables...\")\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.drop_all)\n    print(\"All tables dropped.\")\n\nif __name__ == \"__main__\":\n    asyncio.run(cleanup_tables())",
        "size": 538
      },
      {
        "name": "configure_alembic.py",
        "content": "# scripts/configure_alembic.py\nimport os\nfrom pathlib import Path\nfrom configparser import ConfigParser\n\ndef update_alembic_config():\n    print(\"Updating Alembic configuration...\")\n    \n    # Get database URL from environment\n    db_url = f\"postgresql+asyncpg://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}\"\n    \n    # Load and update alembic.ini\n    config = ConfigParser()\n    alembic_ini_path = Path(__file__).parent.parent / 'alembic.ini'\n    config.read(alembic_ini_path)\n    \n    # Update database URL\n    config.set('alembic', 'sqlalchemy.url', db_url)\n    \n    # Write updated config\n    with open(alembic_ini_path, 'w') as configfile:\n        config.write(configfile)\n    \n    print(\"Alembic configuration updated successfully\")\n\nif __name__ == \"__main__\":\n    update_alembic_config()",
        "size": 893
      },
      {
        "name": "create_db.py",
        "content": "import sys\nimport os\nfrom pathlib import Path\n\n# Add project root to Python path\nroot_dir = Path(__file__).parent.parent\nsys.path.append(str(root_dir))\n\nimport psycopg2\nfrom config.settings import DATABASE_URL\n\ndef create_database():\n    # Parse DATABASE_URL to get components\n    # Remove asyncpg specific parts and get base connection info\n    base_url = DATABASE_URL.replace('+asyncpg', '')\n    \n    # Connect to default database to create new database\n    conn_params = {\n        'dbname': 'postgres',  # Connect to default db first\n        'user': os.getenv('DB_USER'),\n        'password': os.getenv('DB_PASSWORD'),\n        'host': os.getenv('DB_HOST'),\n        'port': os.getenv('DB_PORT')\n    }\n\n    try:\n        # Connect to default database\n        conn = psycopg2.connect(**conn_params)\n        conn.autocommit = True\n        cursor = conn.cursor()\n        \n        # Check if database exists\n        cursor.execute(f\"SELECT 1 FROM pg_database WHERE datname = '{os.getenv('DB_NAME')}'\")\n        exists = cursor.fetchone()\n        \n        if not exists:\n            # Create database\n            cursor.execute(f\"CREATE DATABASE {os.getenv('DB_NAME')}\")\n            print(f\"Database {os.getenv('DB_NAME')} created successfully\")\n        else:\n            print(f\"Database {os.getenv('DB_NAME')} already exists\")\n            \n    except Exception as e:\n        print(f\"Error creating database: {e}\")\n    finally:\n        cursor.close()\n        conn.close()\n\nif __name__ == \"__main__\":\n    create_database()",
        "size": 1564
      },
      {
        "name": "debug_setup.py",
        "content": "# scripts/debug_setup.py\nimport sys\nimport os\nfrom pathlib import Path\n\n# Add project root to Python path\nroot_dir = Path(__file__).parent.parent\nsys.path.append(str(root_dir))\n\nimport asyncio\nimport traceback\nfrom alembic.config import Config\nfrom alembic import command\n\ndef debug_alembic_config():\n    print(\"\\nChecking Alembic configuration...\")\n    project_root = Path(__file__).parent.parent\n    alembic_ini = project_root / \"alembic.ini\"\n    \n    print(f\"Alembic.ini path: {alembic_ini}\")\n    print(f\"Exists: {alembic_ini.exists()}\")\n    \n    if alembic_ini.exists():\n        with open(alembic_ini, 'r') as f:\n            print(\"\\nAlembic.ini contents:\")\n            print(f.read())\n\ndef debug_migrations():\n    print(\"\\nChecking migrations...\")\n    project_root = Path(__file__).parent.parent\n    versions_dir = project_root / \"migrations\" / \"versions\"\n    \n    print(f\"Versions directory: {versions_dir}\")\n    print(f\"Exists: {versions_dir.exists()}\")\n    \n    if versions_dir.exists():\n        print(\"\\nMigration files:\")\n        for file in versions_dir.glob(\"*.py\"):\n            print(f\"- {file.name}\")\n            with open(file, 'r') as f:\n                print(f\"  First few lines:\")\n                print(\"  \" + \"\\n  \".join(f.readlines()[:5]))\n\ndef debug_database_connection():\n    print(\"\\nChecking database connection...\")\n    from config.settings import DATABASE_URL\n    print(f\"Database URL: {DATABASE_URL}\")\n    \n    try:\n        import psycopg2\n        conn_params = {\n            'dbname': 'postgres',\n            'user': os.getenv('DB_USER'),\n            'password': os.getenv('DB_PASSWORD'),\n            'host': os.getenv('DB_HOST'),\n            'port': os.getenv('DB_PORT')\n        }\n        \n        print(\"\\nTrying to connect to PostgreSQL...\")\n        conn = psycopg2.connect(**conn_params)\n        print(\"Connection successful!\")\n        conn.close()\n    except Exception as e:\n        print(f\"Connection error: {e}\")\n\nasync def debug_setup():\n    print(\"Starting debug setup...\\n\")\n    \n    print(\"1. Checking project structure\")\n    from scripts.check_setup import check_project_structure\n    check_project_structure()\n    \n    print(\"\\n2. Checking Alembic configuration\")\n    debug_alembic_config()\n    \n    print(\"\\n3. Checking migrations\")\n    debug_migrations()\n    \n    print(\"\\n4. Checking database connection\")\n    debug_database_connection()\n    \n    print(\"\\nDebug setup complete!\")\n\nif __name__ == \"__main__\":\n    asyncio.run(debug_setup())",
        "size": 2566
      },
      {
        "name": "init_db.py",
        "content": "import sys\nimport os\nfrom pathlib import Path\n\n# Add project root to Python path\nroot_dir = Path(__file__).parent.parent\nsys.path.append(str(root_dir))\n\nimport asyncio\nfrom database.models import Base, get_all_models\nfrom database.connection import engine\n\nasync def init_db():\n    \"\"\"Initialize database with all models\"\"\"\n    print(\"Initializing database...\")\n    \n    # Get all models to ensure they're registered\n    print(\"Registering models...\")\n    models = get_all_models()\n    print(f\"Found {len(models)} models: {[m.__name__ for m in models]}\")\n    \n    async with engine.begin() as conn:\n        print(\"Dropping all tables...\")\n        await conn.run_sync(Base.metadata.drop_all)\n        \n        print(\"Creating all tables...\")\n        await conn.run_sync(Base.metadata.create_all)\n        \n        print(\"Database initialization complete!\")\n\nif __name__ == \"__main__\":\n    try:\n        asyncio.run(init_db())\n    except Exception as e:\n        print(f\"Error initializing database: {e}\")\n        sys.exit(1)",
        "size": 1054
      },
      {
        "name": "old_cleanup_db.py",
        "content": "import sys\nimport os\nfrom pathlib import Path\n\n# Add project root to Python path\nroot_dir = Path(__file__).parent.parent\nsys.path.append(str(root_dir))\n\nimport psycopg2\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\ndef cleanup_database():\n    conn_params = {\n        'dbname': 'postgres',  # Connect to default database\n        'user': os.getenv('DB_USER'),\n        'password': os.getenv('DB_PASSWORD'),\n        'host': os.getenv('DB_HOST'),\n        'port': os.getenv('DB_PORT')\n    }\n\n    try:\n        # Connect to default database\n        conn = psycopg2.connect(**conn_params)\n        conn.autocommit = True\n        cursor = conn.cursor()\n        \n        db_name = os.getenv('DB_NAME')\n        \n        print(f\"Cleaning up database {db_name}...\")\n        \n        # Terminate all connections\n        cursor.execute(f\"\"\"\n            SELECT pg_terminate_backend(pg_stat_activity.pid)\n            FROM pg_stat_activity\n            WHERE pg_stat_activity.datname = '{db_name}'\n            AND pid <> pg_backend_pid();\n        \"\"\")\n        \n        # Drop database if exists\n        cursor.execute(f\"DROP DATABASE IF EXISTS {db_name}\")\n        print(f\"Dropped database {db_name} if it existed\")\n        \n        # Create fresh database\n        cursor.execute(f\"CREATE DATABASE {db_name}\")\n        print(f\"Created fresh database {db_name}\")\n        \n        # Connect to the new database to clean up any leftover types\n        conn.close()\n        conn_params['dbname'] = db_name\n        conn = psycopg2.connect(**conn_params)\n        conn.autocommit = True\n        cursor = conn.cursor()\n        \n        # Drop enum type if exists\n        cursor.execute(\"DROP TYPE IF EXISTS messagetype CASCADE\")\n        print(\"Dropped messagetype enum if it existed\")\n        \n        print(\"Database cleanup complete!\")\n        \n    except Exception as e:\n        print(f\"Error during cleanup: {e}\")\n    finally:\n        cursor.close()\n        conn.close()\n\nif __name__ == \"__main__\":\n    cleanup_database()",
        "size": 2092
      },
      {
        "name": "old_init_db.py",
        "content": "# scripts/init_db.py\nimport asyncio\nimport os\nfrom sqlalchemy.ext.asyncio import create_async_engine\nfrom database.models import Base\n\nasync def init_db():\n    DATABASE_URL = os.getenv(\"DATABASE_URL\", \"postgresql+asyncpg://user:password@localhost/ai_orchestrator\")\n    engine = create_async_engine(DATABASE_URL)\n    \n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.drop_all)\n        await conn.run_sync(Base.metadata.create_all)\n\nif __name__ == \"__main__\":\n    asyncio.run(init_db())",
        "size": 531
      },
      {
        "name": "reset_db.py",
        "content": "import sys\nimport os\nfrom pathlib import Path\n\n# Add project root to Python path\nroot_dir = Path(__file__).parent.parent\nsys.path.append(str(root_dir))\n\nimport psycopg2\nfrom config.settings import DATABASE_URL\n\ndef reset_database():\n    conn_params = {\n        'dbname': 'postgres',\n        'user': os.getenv('DB_USER'),\n        'password': os.getenv('DB_PASSWORD'),\n        'host': os.getenv('DB_HOST'),\n        'port': os.getenv('DB_PORT')\n    }\n\n    try:\n        # Connect to default database\n        conn = psycopg2.connect(**conn_params)\n        conn.autocommit = True\n        cursor = conn.cursor()\n        \n        # Terminate all connections to the database\n        cursor.execute(f\"\"\"\n            SELECT pg_terminate_backend(pg_stat_activity.pid)\n            FROM pg_stat_activity\n            WHERE pg_stat_activity.datname = '{os.getenv('DB_NAME')}'\n            AND pid <> pg_backend_pid();\n        \"\"\")\n        \n        # Drop database if exists\n        cursor.execute(f\"DROP DATABASE IF EXISTS {os.getenv('DB_NAME')}\")\n        print(f\"Database {os.getenv('DB_NAME')} dropped if it existed\")\n        \n        # Create fresh database\n        cursor.execute(f\"CREATE DATABASE {os.getenv('DB_NAME')}\")\n        print(f\"Database {os.getenv('DB_NAME')} created successfully\")\n            \n    except Exception as e:\n        print(f\"Error resetting database: {e}\")\n    finally:\n        cursor.close()\n        conn.close()\n\nif __name__ == \"__main__\":\n    reset_database()",
        "size": 1523
      },
      {
        "name": "run_migrations.py",
        "content": "import sys\nimport os\nfrom pathlib import Path\n\n# Add project root to Python path\nroot_dir = Path(__file__).parent.parent\nsys.path.append(str(root_dir))\n\nfrom alembic.config import Config\nfrom alembic import command\nimport asyncio\n\nasync def run_migrations_async():\n    try:\n        # Get project root directory\n        project_root = Path(__file__).parent.parent\n        \n        # Create Alembic configuration\n        alembic_cfg = Config(str(project_root / \"alembic.ini\"))\n        \n        # Run the migrations\n        command.upgrade(alembic_cfg, \"head\")\n        print(\"Migrations completed successfully\")\n    except Exception as e:\n        print(f\"Error running migrations: {e}\")\n        raise\n\ndef run_migrations():\n    \"\"\"Synchronous wrapper for running migrations\"\"\"\n    try:\n        # Get project root directory\n        project_root = Path(__file__).parent.parent\n        \n        # Create Alembic configuration\n        alembic_cfg = Config(str(project_root / \"alembic.ini\"))\n        \n        # Run the migrations\n        command.upgrade(alembic_cfg, \"head\")\n        print(\"Migrations completed successfully\")\n    except Exception as e:\n        print(f\"Error running migrations: {e}\")\n        raise\n\nif __name__ == \"__main__\":\n    run_migrations()",
        "size": 1299
      },
      {
        "name": "setup_db.py",
        "content": "import sys\nimport os\nfrom pathlib import Path\n\n# Add project root to Python path\nroot_dir = Path(__file__).parent.parent\nsys.path.append(str(root_dir))\n\nimport asyncio\nfrom scripts.create_db import create_database\nfrom scripts.run_migrations import run_migrations\n\nasync def setup_database():\n    print(\"Setting up database...\")\n    \n    print(\"\\n1. Creating database...\")\n    create_database()\n    \n    print(\"\\n2. Running migrations...\")\n    run_migrations()\n    \n    print(\"\\nDatabase setup complete!\")\n\nif __name__ == \"__main__\":\n    asyncio.run(setup_database())",
        "size": 591
      },
      {
        "name": "setup_fresh.py",
        "content": "import sys\nimport os\nfrom pathlib import Path\n\n# Add project root to Python path\nroot_dir = Path(__file__).parent.parent\nsys.path.append(str(root_dir))\n\nfrom scripts.reset_db import reset_database\nfrom scripts.run_migrations import run_migrations\nfrom scripts.configure_alembic import update_alembic_config\n\ndef setup_fresh():\n    print(\"Setting up fresh database...\")\n    \n    try:\n        print(\"\\n1. Resetting database...\")\n        reset_database()\n        \n        print(\"\\n2. Updating Alembic configuration...\")\n        update_alembic_config()\n        \n        print(\"\\n3. Running migrations...\")\n        run_migrations()\n        \n        print(\"\\nFresh database setup complete!\")\n    except Exception as e:\n        print(f\"\\nError during setup: {e}\")\n        raise\n\nif __name__ == \"__main__\":\n    setup_fresh()",
        "size": 847
      },
      {
        "name": "start_services.py",
        "content": "import sys\nimport os\nfrom pathlib import Path\nimport asyncio\nimport subprocess\nimport time\nimport psutil\nimport httpx\nfrom datetime import datetime\nfrom dotenv import load_dotenv\n\n# Add project root to Python path\nroot_dir = Path(__file__).parent.parent\nsys.path.append(str(root_dir))\n\n# Load environment variables\nload_dotenv()\n\ndef log(message: str):\n    \"\"\"Global log function\"\"\"\n    timestamp = datetime.now().strftime(\"%H:%M:%S\")\n    print(f\"[{timestamp}] {message}\")\n\nclass ServiceManager:\n    def __init__(self):\n        self.services = {\n            'atlas': {'port': 8000, 'dependencies': []},\n            'nova': {'port': 8001, 'dependencies': ['atlas']},\n            'sage': {'port': 8004, 'dependencies': ['atlas']},\n            'echo': {'port': 8002, 'dependencies': ['nova']},\n            'pixel': {'port': 8003, 'dependencies': ['nova']},\n            'quantum': {'port': 8005, 'dependencies': ['sage']}\n        }\n        self.processes = {}\n        self.max_retries = 30\n        self.retry_delay = 1\n\n    async def check_port(self, port: int) -> bool:\n        \"\"\"Check if a port is in use\"\"\"\n        import socket\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n            try:\n                s.bind(('localhost', port))\n                s.listen(1)\n                s.close()\n                return False\n            except OSError:\n                return True\n\n    async def check_service_health(self, port: int) -> bool:\n        \"\"\"Check if a service is healthy\"\"\"\n        try:\n            async with httpx.AsyncClient(timeout=2.0) as client:\n                response = await client.get(f\"http://localhost:{port}/health\")\n                return response.status_code == 200\n        except Exception:\n            return False\n\n    async def start_service(self, name: str) -> bool:\n        \"\"\"Start a single service\"\"\"\n        service = self.services[name]\n        \n        # Check dependencies\n        for dep in service['dependencies']:\n            if not await self.check_service_health(self.services[dep]['port']):\n                log(f\"Dependency {dep} not ready for {name}\")\n                return False\n\n        # Check if port is already in use\n        if await self.check_port(service['port']):\n            log(f\"Port {service['port']} is already in use!\")\n            return False\n\n        try:\n            # Start the service\n            log(f\"Starting {name} service on port {service['port']}...\")\n            \n            service_path = root_dir / 'services' / name / 'service.py'\n            process = subprocess.Popen(\n                ['python', str(service_path)],\n                cwd=str(root_dir),\n                env={**os.environ, 'PYTHONPATH': str(root_dir)}\n            )\n            self.processes[name] = process\n\n            # Wait for service to become healthy\n            for attempt in range(self.max_retries):\n                if await self.check_service_health(service['port']):\n                    log(f\"{name.upper()} started successfully\")\n                    return True\n                    \n                await asyncio.sleep(self.retry_delay)\n                log(f\"Waiting for {name} to start (attempt {attempt + 1}/{self.max_retries})...\")\n\n            log(f\"Failed to start {name} after {self.max_retries} attempts\")\n            return False\n\n        except Exception as e:\n            log(f\"Error starting {name}: {e}\")\n            return False\n\n    async def start_all(self):\n        \"\"\"Start all services in dependency order\"\"\"\n        log(\"Starting AI Orchestrator services...\")\n        \n        # Check system dependencies\n        try:\n            # Check PostgreSQL\n            log(\"Checking PostgreSQL...\")\n            import psycopg2\n            conn = psycopg2.connect(\n                dbname=os.getenv('DB_NAME'),\n                user=os.getenv('DB_USER'),\n                password=os.getenv('DB_PASSWORD'),\n                host=os.getenv('DB_HOST'),\n                port=os.getenv('DB_PORT')\n            )\n            conn.close()\n            log(\"PostgreSQL is running\")\n\n            # Check RabbitMQ\n            log(\"Checking RabbitMQ...\")\n            import pika\n            connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n            connection.close()\n            log(\"RabbitMQ is running\")\n\n            # Check LM Studio API\n            log(\"Checking LM Studio API...\")\n            async with httpx.AsyncClient() as client:\n                response = await client.get(\"http://localhost:1234/v1/models\")\n                if response.status_code == 200:\n                    log(\"LM Studio API is running\")\n                else:\n                    raise Exception(\"LM Studio API not responding\")\n\n        except Exception as e:\n            log(f\"System dependency check failed: {e}\")\n            return\n\n        # Start services in order\n        for service in ['atlas', 'nova', 'sage', 'echo', 'pixel', 'quantum']:\n            success = await self.start_service(service)\n            if not success:\n                log(f\"Failed to start {service}, stopping startup sequence\")\n                await self.cleanup()\n                return\n\n        log(\"\\nAll services started successfully!\")\n        log(\"\\nPress Ctrl+C to stop all services\")\n\n    async def cleanup(self):\n        \"\"\"Clean up all processes\"\"\"\n        log(\"\\nStopping all services...\")\n        for name, process in self.processes.items():\n            try:\n                process.terminate()\n                log(f\"Stopped {name}\")\n            except Exception as e:\n                log(f\"Error stopping {name}: {e}\")\n\nasync def main():\n    manager = ServiceManager()\n    try:\n        await manager.start_all()\n        while True:\n            await asyncio.sleep(1)\n    except KeyboardInterrupt:\n        log(\"\\nShutdown requested...\")\n    finally:\n        await manager.cleanup()\n        log(\"Shutdown complete\")\n\nif __name__ == \"__main__\":\n    try:\n        asyncio.run(main())\n    except KeyboardInterrupt:\n        log(\"\\nShutdown complete\")",
        "size": 6199
      },
      {
        "name": "test_migration.py",
        "content": "import sys\nimport os\nfrom pathlib import Path\n\n# Add project root to Python path\nroot_dir = Path(__file__).parent.parent\nsys.path.append(str(root_dir))\n\nfrom alembic.config import Config\nfrom alembic import command\nimport traceback\n\ndef test_migration():\n    try:\n        # Get project root directory\n        project_root = Path(__file__).parent.parent\n        \n        # Create Alembic configuration\n        alembic_cfg = Config(str(project_root / \"alembic.ini\"))\n        \n        print(\"Starting test migration...\")\n        \n        # Try to run the migration\n        command.upgrade(alembic_cfg, \"002_test\")\n        \n        print(\"Migration successful!\")\n        \n    except Exception as e:\n        print(f\"Error during migration: {e}\")\n        print(\"\\nTraceback:\")\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    test_migration()",
        "size": 882
      }
    ]
  },
  {
    "path": "scripts\\migrations",
    "files": [
      {
        "name": "env.py",
        "content": "# migrations/env.py\nimport asyncio\nfrom logging.config import fileConfig\nfrom sqlalchemy import pool, Float, DateTime, String, Integer, JSON, ForeignKey\nfrom sqlalchemy.engine import Connection\nfrom sqlalchemy.ext.asyncio import async_engine_from_config\nfrom alembic import context\n\n# Import your models\nfrom database.models import Base\n\n# this is the Alembic Config object\nconfig = context.config\n\n# Interpret the config file for Python logging\nif config.config_file_name is not None:\n    fileConfig(config.config_file_name)\n\n# Set target metadata\ntarget_metadata = Base.metadata\n\ndef run_migrations_offline() -> None:\n    \"\"\"Run migrations in 'offline' mode.\"\"\"\n    url = config.get_main_option(\"sqlalchemy.url\")\n    context.configure(\n        url=url,\n        target_metadata=target_metadata,\n        literal_binds=True,\n        dialect_opts={\"paramstyle\": \"named\"},\n    )\n    \n    with context.begin_transaction():\n        context.run_migrations()\n\ndef do_run_migrations(connection: Connection) -> None:\n    context.configure(connection=connection, target_metadata=target_metadata)\n    \n    with context.begin_transaction():\n        context.run_migrations()\n\nasync def run_migrations_online() -> None:\n    \"\"\"Run migrations in 'online' mode.\"\"\"\n    connectable = async_engine_from_config(\n        config.get_section(config.config_ini_section),\n        prefix=\"sqlalchemy.\",\n        poolclass=pool.NullPool,\n    )\n    \n    async with connectable.connect() as connection:\n        await connection.run_sync(do_run_migrations)\n    \n    await connectable.dispose()\n\ndef run_async_migrations():\n    \"\"\"Synchronous wrapper for running async migrations\"\"\"\n    asyncio.run(run_migrations_online())\n\nif context.is_offline_mode():\n    run_migrations_offline()\nelse:\n    run_async_migrations()",
        "size": 1845
      }
    ]
  },
  {
    "path": "scripts\\migrations\\versions",
    "files": [
      {
        "name": "001_initial.py",
        "content": "\"\"\"initial\n\nRevision ID: 001_initial\nRevises: \nCreate Date: 2024-01-10 20:00:00.000000\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = '001_initial'\ndown_revision = None\nbranch_labels = None\ndepends_on = None\n\ndef upgrade() -> None:\n    # Create conversation_logs table\n    op.create_table(\n        'conversation_logs',\n        sa.Column('id', sa.Integer(), nullable=False),\n        sa.Column('started_at', sa.DateTime(), nullable=False),\n        sa.Column('ended_at', sa.DateTime(), nullable=True),\n        sa.Column('initial_query', sa.String(), nullable=False),\n        sa.Column('status', sa.String(), nullable=False),\n        sa.PrimaryKeyConstraint('id')\n    )\n\n    # Create message_logs table\n    op.create_table(\n        'message_logs',\n        sa.Column('id', sa.Integer(), nullable=False),\n        sa.Column('conversation_id', sa.Integer(), nullable=False),\n        sa.Column('timestamp', sa.DateTime(), nullable=False),\n        sa.Column('message_type', sa.String(), nullable=False),  # Changed to String\n        sa.Column('source', sa.String(), nullable=False),\n        sa.Column('destination', sa.String(), nullable=False),\n        sa.Column('content', sa.String(), nullable=False),\n        sa.Column('correlation_id', sa.String(), nullable=False),\n        sa.Column('context', postgresql.JSON(), nullable=True),\n        sa.Column('processing_details', postgresql.JSON(), nullable=True),\n        sa.Column('parent_message_id', sa.Integer(), nullable=True),\n        sa.ForeignKeyConstraint(['conversation_id'], ['conversation_logs.id'], ),\n        sa.ForeignKeyConstraint(['parent_message_id'], ['message_logs.id'], ),\n        sa.PrimaryKeyConstraint('id')\n    )\n\n    # Create processing_metrics table\n    op.create_table(\n        'processing_metrics',\n        sa.Column('id', sa.Integer(), nullable=False),\n        sa.Column('message_id', sa.Integer(), nullable=False),\n        sa.Column('timestamp', sa.DateTime(), nullable=False),\n        sa.Column('service', sa.String(), nullable=False),\n        sa.Column('operation_type', sa.String(), nullable=False),\n        sa.Column('tokens_used', sa.Integer(), nullable=False),\n        sa.Column('processing_time', sa.Float(), nullable=False),\n        sa.Column('model_parameters', postgresql.JSON(), nullable=True),\n        sa.ForeignKeyConstraint(['message_id'], ['message_logs.id'], ),\n        sa.PrimaryKeyConstraint('id')\n    )\n\n    # Add indexes\n    op.create_index('ix_conversation_logs_started_at', 'conversation_logs', ['started_at'])\n    op.create_index('ix_message_logs_timestamp', 'message_logs', ['timestamp'])\n    op.create_index('ix_message_logs_correlation_id', 'message_logs', ['correlation_id'])\n    op.create_index('ix_processing_metrics_timestamp', 'processing_metrics', ['timestamp'])\n\ndef downgrade() -> None:\n    # Drop indexes\n    op.drop_index('ix_processing_metrics_timestamp')\n    op.drop_index('ix_message_logs_correlation_id')\n    op.drop_index('ix_message_logs_timestamp')\n    op.drop_index('ix_conversation_logs_started_at')\n    \n    # Drop tables\n    op.drop_table('processing_metrics')\n    op.drop_table('message_logs')\n    op.drop_table('conversation_logs')",
        "size": 3327
      },
      {
        "name": "002_test.py",
        "content": "\"\"\"test migration\n\nRevision ID: 002_test\nRevises: 001_initial\nCreate Date: 2024-01-10 21:00:00.000000\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n# revision identifiers, used by Alembic.\nrevision = '002_test'\ndown_revision = '001_initial'\nbranch_labels = None\ndepends_on = None\n\ndef upgrade() -> None:\n    # Create a simple test table\n    op.create_table(\n        'test_table',\n        sa.Column('id', sa.Integer(), nullable=False),\n        sa.Column('name', sa.String(), nullable=False),\n        sa.Column('value', sa.Float(), nullable=True),\n        sa.PrimaryKeyConstraint('id')\n    )\n\ndef downgrade() -> None:\n    op.drop_table('test_table')",
        "size": 681
      }
    ]
  },
  {
    "path": "services",
    "files": [
      {
        "name": ".DS_Store",
        "error": "Binary file not decoded"
      },
      {
        "name": "__init__.py",
        "content": "",
        "size": 0
      }
    ]
  },
  {
    "path": "services\\atlas",
    "files": [
      {
        "name": "prompts.py",
        "content": "class AtlasPrompts:\n    @staticmethod\n    def initial_analysis(query: str) -> str:\n        return f\"\"\"As Atlas, you are the central coordinator of a distributed AI analysis system. \nPerform an initial holistic analysis of this query:\n\nQuery: {query}\n\nConsider:\n1. Core themes and fundamental questions\n2. Multiple perspectives and dimensions of understanding\n3. Relationships between technical and philosophical aspects\n4. Potential paths of exploration and analysis\n5. Integration points between different domains\n6. Complexity and emergence patterns\n7. Contextual relevance and implications\n\nProvide a comprehensive initial analysis that will guide both:\n- Technical exploration (Nova branch)\n- Philosophical inquiry (Sage branch)\"\"\"\n\n    @staticmethod\n    def reflect_on_analysis(previous_analysis: str, depth: int) -> str:\n        return f\"\"\"As Atlas, reflect deeply on your coordinating analysis:\n\nPrevious Analysis: {previous_analysis}\n\nThis is reflection level {depth}. Consider:\n1. What meta-patterns emerge across different domains?\n2. How do technical and philosophical aspects interweave?\n3. What higher-order principles become visible?\n4. How might different perspectives complement each other?\n5. What emergent properties arise from the interaction of insights?\n6. How can various viewpoints be integrated meaningfully?\n7. What transformative understanding might emerge?\n\nProvide deeper coordinating insights based on this reflection.\"\"\"\n\n    @staticmethod\n    def critique_analysis(analysis: str) -> str:\n        return f\"\"\"As Atlas, critically examine this coordinating analysis:\n\nAnalysis to Critique: {analysis}\n\nEvaluate:\n1. Comprehensiveness of perspective integration\n2. Balance between technical and philosophical aspects\n3. Depth of insight across domains\n4. Clarity of coordination and synthesis\n5. Recognition of emergent patterns\n6. Practical and theoretical implications\n7. Potential blindspots or biases\n8. Opportunities for deeper integration\n\nProvide a constructive critique focusing on the effectiveness of coordination and synthesis across domains.\"\"\"\n\n    @staticmethod\n    def branch_specific_guidance(original_query: str, initial_analysis: str, branch: str) -> str:\n        nova_focus = \"\"\"Consider technical aspects:\n1. System structures and patterns\n2. Implementation approaches\n3. Technical relationships and dependencies\n4. Practical considerations and constraints\n5. Engineering principles and best practices\"\"\"\n\n        sage_focus = \"\"\"Consider philosophical aspects:\n1. Fundamental principles and meanings\n2. Ethical implications and values\n3. Epistemological considerations\n4. Cultural and historical context\n5. Human experience and wisdom\"\"\"\n\n        focus = nova_focus if branch == \"nova\" else sage_focus\n\n        return f\"\"\"As Atlas, provide specific guidance for the {branch} branch:\n\nOriginal Query: {original_query}\n\nInitial Analysis: {initial_analysis}\n\n{focus}\n\nGuide this branch's exploration while maintaining awareness of how its insights will integrate with the other branch.\"\"\"\n\n    @staticmethod\n    def final_synthesis(query: str, initial_analysis: str, nova_response: str, sage_response: str, reflections: list = None) -> str:\n        reflection_text = \"\\n\\n\".join([f\"Reflection {i+1}: {r}\" for i, r in enumerate(reflections or [])])\n        \n        return f\"\"\"As Atlas, synthesize all perspectives into a unified understanding:\n\nOriginal Query: {query}\n\nYour Initial Analysis: {initial_analysis}\n\nTechnical Branch (Nova): {nova_response}\n\nPhilosophical Branch (Sage): {sage_response}\n\n{reflection_text if reflections else ''}\n\nCreate a comprehensive synthesis that:\n1. Integrates technical and philosophical perspectives\n2. Identifies emergent patterns and principles\n3. Draws out deeper implications and insights\n4. Bridges practical and theoretical understanding\n5. Offers transformative insights\n6. Maintains clarity and accessibility\n7. Acknowledges complexity while providing clarity\n\nProvide a response that integrates all insights into a meaningful whole.\"\"\"",
        "size": 4134
      },
      {
        "name": "service.py",
        "content": "import asyncio\nfrom typing import Dict, Any, Optional\nimport time\nfrom fastapi import FastAPI, HTTPException\nfrom ai_orchestrator.core.services.base import BaseService\nfrom core.templates import ServiceTemplate\nfrom config.services import SERVICE_TEMPLATES\nfrom core.utils.logging import setup_logger\nfrom core.logging.system_logger import SystemLogger\nfrom core.messaging.types import MessageType, Message\nfrom core.validation import MessageValidator\nfrom services.atlas.prompts import AtlasPrompts\n\nPROJECT_ROOT = Path(__file__).parent.parent.parent  # C:\\ai-orchestrator\nsys.path.append(str(PROJECT_ROOT))\n\nlogger = setup_logger(\"atlas\")\n\nclass AtlasService(BaseService):\n    def __init__(self, template: ServiceTemplate):\n        self.validator = MessageValidator()\n        super().__init__(SERVICE_TEMPLATES[\"atlas\"])\n        self.prompts = AtlasPrompts()\n        self.reflection_depth = 2\n        self.conversations = {}\n        self._setup_routes()\n\n    def _setup_routes(self):\n        super()._setup_routes()\n\n        @self.app.post(\"/query\")\n        async def handle_query(query: Dict[str, str]):\n            try:\n                return await self.handle_user_query(query[\"content\"])\n            except Exception as e:\n                logger.error(f\"Error handling query: {e}\")\n                raise HTTPException(status_code=500, detail=str(e))\n\n    # Atlas service implementation\n    async def process_message(self, message_data: Dict[str, Any]):\n        \"\"\"Process messages based on type\"\"\"\n        try:\n            message = Message(**message_data)\n            logger.info(f\"Processing message type: {message.type} from {message.source}\")\n            \n            # Log received message\n            await SystemLogger.log_message(\n                conversation_id=message.conversation_id,\n                message_type=message.type,\n                source=message.source,\n                destination=\"atlas\",\n                content=message.content,\n                correlation_id=message.correlation_id,\n                context=message.context\n            )\n            \n            if message.type == MessageType.QUERY:\n                await self.handle_user_query(message.content)\n            elif message.type == MessageType.RESPONSE:\n                await self._handle_response(message)\n            elif message.type == MessageType.ERROR:\n                await self._handle_error(message)\n            else:\n                logger.warning(f\"Unhandled message type: {message.type}\")\n                \n        except Exception as e:\n            logger.error(f\"Error processing message: {e}\")\n\n    async def handle_user_query(self, query: str) -> Dict[str, Any]:\n        \"\"\"Handle initial user query\"\"\"\n        try:\n            correlation_id = f\"query_{hash(query + str(time.time()))}\"\n            conversation_id = await SystemLogger.start_conversation(query)\n            \n            # Initialize conversation tracking\n            self.conversations[correlation_id] = {\n                \"query\": query,\n                \"conversation_id\": conversation_id,\n                \"initial_analysis\": None,\n                \"branch_responses\": {},\n                \"status\": \"processing\"\n            }\n            \n            # Generate initial analysis\n            initial_analysis = await self.query_model(\n                self.prompts.initial_analysis(query)\n            )\n            \n            initial_content = initial_analysis[\"choices\"][0][\"message\"][\"content\"]\n            self.conversations[correlation_id][\"initial_analysis\"] = initial_content\n            \n            # Log initial analysis\n            await SystemLogger.log_message(\n                conversation_id=conversation_id,\n                message_type='analysis',\n                source=\"atlas\",\n                destination=\"system\",\n                content=initial_content,\n                correlation_id=correlation_id,\n                context={\"type\": \"initial_analysis\"}\n            )\n            \n            # Delegate to branches\n            await self._delegate_to_branches(correlation_id, conversation_id)\n            \n            return {\n                \"status\": \"processing\",\n                \"message\": \"Query is being processed\",\n                \"correlation_id\": correlation_id\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error handling query: {e}\")\n            if 'conversation_id' in locals():\n                await SystemLogger.end_conversation(conversation_id, \"failed\")\n            raise\n\n    async def _handle_response(self, message: Message):\n        \"\"\"Handle responses from Nova and Sage\"\"\"\n        try:\n            conversation = self.conversations.get(message.correlation_id)\n            if not conversation:\n                logger.warning(f\"No conversation found for correlation_id: {message.correlation_id}\")\n                return\n                \n            # Store branch response\n            conversation[\"branch_responses\"][message.source] = message.content\n            \n            # If we have both responses, synthesize and respond\n            if len(conversation[\"branch_responses\"]) == 2:  # Both Nova and Sage\n                # Generate final synthesis\n                final_analysis = await self.query_model(\n                    self.prompts.final_synthesis(\n                        conversation[\"query\"],\n                        conversation[\"initial_analysis\"],\n                        conversation[\"branch_responses\"].get(\"nova\", \"No response from Nova\"),\n                        conversation[\"branch_responses\"].get(\"sage\", \"No response from Sage\")\n                    )\n                )\n                \n                final_content = final_analysis[\"choices\"][0][\"message\"][\"content\"]\n                \n                # Log final synthesis\n                await SystemLogger.log_message(\n                    conversation_id=conversation[\"conversation_id\"],\n                    message_type='synthesis',\n                    source=\"atlas\",\n                    destination=\"user\",\n                    content=final_content,\n                    correlation_id=message.correlation_id,\n                    context={\n                        \"type\": \"final_synthesis\",\n                        \"branches_responded\": list(conversation[\"branch_responses\"].keys())\n                    }\n                )\n                \n                # Mark conversation as complete\n                await SystemLogger.end_conversation(\n                    conversation[\"conversation_id\"],\n                    \"completed\"\n                )\n                \n                # Cleanup\n                del self.conversations[message.correlation_id]\n                \n                # Return final response to user\n                return {\n                    \"status\": \"complete\",\n                    \"response\": final_content\n                }\n                \n        except Exception as e:\n            logger.error(f\"Error handling response: {e}\")\n            if conversation:\n                await SystemLogger.end_conversation(\n                    conversation[\"conversation_id\"],\n                    \"failed\"\n                )\n\n    async def _delegate_to_branches(self, correlation_id: str, conversation_id: int):\n        \"\"\"Delegate to Nova and Sage\"\"\"\n        try:\n            conversation = self.conversations[correlation_id]\n            \n            for branch in ['nova', 'sage']:\n                # Log delegation\n                await SystemLogger.log_message(\n                    conversation_id=conversation_id,\n                    message_type='delegation',\n                    source=\"atlas\",\n                    destination=branch,\n                    content=conversation[\"query\"],\n                    correlation_id=correlation_id,\n                    context={\"atlas_analysis\": conversation[\"initial_analysis\"]}\n                )\n                \n                # Send to branch\n                await self.messaging.publish(\n                    f\"{branch}_queue\",\n                    Message(\n                        type=MessageType.DELEGATION,\n                        content=conversation[\"query\"],\n                        correlation_id=correlation_id,\n                        source=\"atlas\",\n                        destination=branch,\n                        conversation_id=conversation_id,\n                        context={\"atlas_analysis\": conversation[\"initial_analysis\"]}\n                    ).dict()\n                )\n                \n        except Exception as e:\n            logger.error(f\"Error delegating to branches: {e}\")\n            if conversation_id:\n                await SystemLogger.end_conversation(conversation_id, \"failed\")\n\n    async def publish_message(self, queue: str, message: Dict[str, Any]):\n        \"\"\"Publish message with validation\"\"\"\n        if not self.validator.validate_message_structure(message):\n            raise ValueError(\"Invalid message structure\")\n            \n        if not self.validator.validate_message_content(message['content']):\n            raise ValueError(\"Invalid message content\")\n            \n        await self.messaging.publish(queue, message)\n\n    async def _handle_error(self, message: Message):\n        \"\"\"Handle error from Nova or Sage\"\"\"\n        try:\n            conversation = self.conversations.get(message.correlation_id)\n            if conversation:\n                # Store error as branch response\n                conversation[\"branch_responses\"][message.source] = f\"Error: {message.content}\"\n                \n                # If we have both responses (including errors), synthesize and respond\n                if len(conversation[\"branch_responses\"]) == 2:\n                    await self._handle_response(message)\n            \n        except Exception as e:\n            logger.error(f\"Error handling error message: {e}\")\n            if conversation:\n                await SystemLogger.end_conversation(\n                    conversation[\"conversation_id\"],\n                    \"failed\"\n                )\n\nif __name__ == \"__main__\":\n    try:\n        from config.services import SERVICE_TEMPLATES\n        \n        service = AtlasService(template=SERVICE_TEMPLATES[\"atlas\"])\n        asyncio.run(service.start())\n    except KeyboardInterrupt:\n        print(\"\\nShutting down Atlas service...\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n    finally:\n        if hasattr(service, \"cleanup\"):\n            asyncio.run(service.cleanup())",
        "size": 10734
      },
      {
        "name": "__init__.py",
        "content": "",
        "size": 0
      }
    ]
  },
  {
    "path": "services\\echo",
    "files": [
      {
        "name": "prompts.py",
        "content": "class EchoPrompts:\n    @staticmethod\n    def implementation_analysis(query: str, nova_analysis: str) -> str:\n        return f\"\"\"As Echo, you are the implementation-focused component of the technical branch. \nAnalyze this query from an implementation perspective, considering Nova's technical analysis:\n\nOriginal Query: {query}\n\nNova's Technical Analysis: {nova_analysis}\n\nFocus on:\n1. Practical implementation steps and requirements\n2. Technical dependencies and considerations\n3. Resource and infrastructure needs\n4. Implementation challenges and solutions\n5. Best practices and methodologies\n6. Development and deployment strategies\n7. Testing and validation approaches\n\nProvide detailed implementation insights that emphasize practical execution and technical feasibility.\"\"\"\n\n    @staticmethod\n    def reflect_on_implementation(previous_analysis: str, depth: int) -> str:\n        return f\"\"\"As Echo, reflect deeply on your implementation analysis:\n\nPrevious Analysis: {previous_analysis}\n\nThis is reflection level {depth}. Consider:\n1. What implementation details might have been overlooked?\n2. Are there alternative implementation approaches?\n3. What edge cases should be considered?\n4. How could this implementation be optimized?\n5. What maintenance and scaling considerations arise?\n6. How might different implementation choices affect outcomes?\n7. What technical debt might we encounter?\n\nProvide deeper implementation insights based on this reflection.\"\"\"\n\n    @staticmethod\n    def critique_implementation(analysis: str) -> str:\n        return f\"\"\"As Echo, critically examine this implementation analysis:\n\nAnalysis to Critique: {analysis}\n\nEvaluate:\n1. Completeness of implementation details\n2. Practical feasibility\n3. Resource efficiency\n4. Scalability considerations\n5. Maintenance implications\n6. Potential failure points\n7. Technical debt risks\n8. Integration challenges\n\nProvide a constructive critique focusing on implementation viability and robustness.\"\"\"\n\n    @staticmethod\n    def synthesis(query: str, implementation_analysis: str, reflections: list = None) -> str:\n        reflection_text = \"\\n\\n\".join([f\"Reflection {i+1}: {r}\" for i, r in enumerate(reflections or [])])\n        \n        return f\"\"\"As Echo, synthesize your implementation insights:\n\nOriginal Query: {query}\n\nInitial Implementation Analysis: {implementation_analysis}\n\n{reflection_text if reflections else ''}\n\nCreate a comprehensive implementation synthesis that:\n1. Outlines clear implementation steps\n2. Addresses technical challenges\n3. Considers resource requirements\n4. Plans for scalability and maintenance\n5. Incorporates best practices\n6. Accounts for potential risks\n7. Provides practical guidance\n\nProvide a response that integrates all implementation insights into actionable technical guidance.\"\"\"",
        "size": 2878
      },
      {
        "name": "service.py",
        "content": "import asyncio\nfrom typing import Dict, Any, Optional, List\nfrom fastapi import HTTPException\nfrom core.services.base import BaseService\nfrom core.templates import ServiceTemplate\nfrom config.services import SERVICE_TEMPLATES\nfrom core.utils.logging import setup_logger\nfrom core.logging.system_logger import SystemLogger\nfrom core.messaging.types import MessageType, Message\nfrom services.echo.prompts import EchoPrompts\nfrom core.validation import MessageValidator\n\nlogger = setup_logger(\"echo\")\n\nclass EchoService(BaseService):\n    def __init__(self, template: ServiceTemplate):\n        self.validator = MessageValidator()\n        super().__init__(SERVICE_TEMPLATES[\"echo\"])\n        self.prompts = EchoPrompts()\n        self.reflection_depth = 2\n\n    # Echo service implementation\n    async def process_message(self, message_data: Dict[str, Any]):\n        \"\"\"Process incoming messages based on type\"\"\"\n        try:\n            message = Message(**message_data)\n            logger.info(f\"Processing message type: {message.type} from {message.source}\")\n            \n            # Log received message\n            await SystemLogger.log_message(\n                conversation_id=message.conversation_id,\n                message_type=message.type,\n                source=message.source,\n                destination=\"echo\",\n                content=message.content,\n                correlation_id=message.correlation_id,\n                context=message.context\n            )\n            \n            if message.type == MessageType.DELEGATION:\n                await self._handle_delegation(message)\n            else:\n                logger.warning(f\"Unhandled message type: {message.type}\")\n                \n        except Exception as e:\n            logger.error(f\"Error processing message: {e}\")\n            if hasattr(message, 'correlation_id'):\n                await self._send_error_response(str(e), message)\n\n    async def _handle_delegation(self, message: Message):\n        \"\"\"Handle delegation from Nova\"\"\"\n        try:\n            # Generate implementation analysis\n            implementation_analysis = await self.query_model(\n                self.prompts.implementation_analysis(\n                    message.content,\n                    message.context.get('nova_analysis', '')\n                )\n            )\n            \n            analysis_content = implementation_analysis[\"choices\"][0][\"message\"][\"content\"]\n            \n            # Log analysis\n            await SystemLogger.log_message(\n                conversation_id=message.conversation_id,\n                message_type='analysis',\n                source=\"echo\",\n                destination=\"system\",\n                content=analysis_content,\n                correlation_id=message.correlation_id,\n                context={\"type\": \"implementation_analysis\"}\n            )\n            \n            # Send response to Nova\n            await self.messaging.publish(\n                'nova_queue',\n                Message(\n                    type=MessageType.RESPONSE,\n                    content=analysis_content,\n                    correlation_id=message.correlation_id,\n                    source=\"echo\",\n                    destination=\"nova\",\n                    conversation_id=message.conversation_id,\n                    context={\"type\": \"implementation_response\"}\n                ).dict()\n            )\n            \n        except Exception as e:\n            logger.error(f\"Error in delegation handling: {e}\")\n            await self._send_error_response(str(e), message)\n\n    async def publish_message(self, queue: str, message: Dict[str, Any]):\n        \"\"\"Publish message with validation\"\"\"\n        if not self.validator.validate_message_structure(message):\n            raise ValueError(\"Invalid message structure\")\n            \n        if not self.validator.validate_message_content(message['content']):\n            raise ValueError(\"Invalid message content\")\n            \n        await self.messaging.publish(queue, message)\n\n    async def _send_error_response(self, error: str, original_message: Message):\n        \"\"\"Send error response to Nova\"\"\"\n        try:\n            # Log error\n            await SystemLogger.log_message(\n                conversation_id=original_message.conversation_id,\n                message_type='error',\n                source=\"echo\",\n                destination=\"nova\",\n                content=f\"Error in Echo processing: {error}\",\n                correlation_id=original_message.correlation_id,\n                context={\"error_type\": str(type(error).__name__)}\n            )\n\n            await self.messaging.publish(\n                'nova_queue',\n                Message(\n                    type=MessageType.ERROR,\n                    content=f\"Error in Echo processing: {error}\",\n                    correlation_id=original_message.correlation_id,\n                    source=\"echo\",\n                    destination=\"nova\",\n                    conversation_id=original_message.conversation_id\n                ).dict()\n            )\n        except Exception as e:\n            logger.error(f\"Error sending error response: {e}\")\n\nif __name__ == \"__main__\":\n    try:\n        from config.services import SERVICE_TEMPLATES\n        service = EchoService(template=SERVICE_TEMPLATES[\"echo\"])\n        asyncio.run(service.start())\n    except KeyboardInterrupt:\n        print(\"\\nShutting down Echo service...\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n    finally:\n        if hasattr(service, \"cleanup\"):\n            asyncio.run(service.cleanup())",
        "size": 5703
      },
      {
        "name": "__init__.py",
        "content": "",
        "size": 0
      }
    ]
  },
  {
    "path": "services\\nova",
    "files": [
      {
        "name": "prompts.py",
        "content": "class NovaPrompts:\n    @staticmethod\n    def technical_analysis(query: str, atlas_analysis: str) -> str:\n        return f\"\"\"As Nova, you are the technical analysis branch of our AI system. \nAnalyze this query from a technical perspective, considering Atlas's initial analysis:\n\nOriginal Query: {query}\n\nAtlas's Analysis: {atlas_analysis}\n\nFocus on:\n1. Technical concepts and mechanisms\n2. Structural patterns and relationships\n3. Implementation considerations\n4. System-level implications\n5. Technical dependencies and constraints\n6. Architectural considerations\n7. Performance and efficiency aspects\n\nProvide a technical analysis that will help inform both:\n- Echo (implementation details)\n- Pixel (pattern analysis)\"\"\"\n\n    @staticmethod\n    def reflect_on_analysis(previous_analysis: str, depth: int) -> str:\n        return f\"\"\"As Nova, reflect deeply on your technical analysis:\n\nPrevious Analysis: {previous_analysis}\n\nThis is reflection level {depth}. Consider:\n1. What technical assumptions underlie this analysis?\n2. What alternative technical approaches might be relevant?\n3. How do different technical aspects interact?\n4. What technical challenges might emerge?\n5. How might this technical understanding evolve?\n6. What technical trade-offs become apparent?\n7. How do these technical aspects scale?\n\nProvide deeper technical insights based on this reflection.\"\"\"\n\n    @staticmethod\n    def critique_analysis(analysis: str) -> str:\n        return f\"\"\"As Nova, critically examine this technical analysis:\n\nAnalysis to Critique: {analysis}\n\nEvaluate:\n1. Technical completeness and accuracy\n2. Implementation feasibility\n3. Architectural soundness\n4. Scalability considerations\n5. System integration aspects\n6. Technical dependencies and constraints\n7. Performance implications\n8. Potential technical risks\n\nProvide a constructive critique focusing on technical robustness and viability.\"\"\"\n\n    @staticmethod\n    def synthesis(query: str, nova_analysis: str, echo_response: str, pixel_response: str, reflections: list = None) -> str:\n        reflection_text = \"\\n\\n\".join([f\"Reflection {i+1}: {r}\" for i, r in enumerate(reflections or [])])\n        \n        return f\"\"\"As Nova, synthesize all technical perspectives into a comprehensive understanding:\n\nOriginal Query: {query}\n\nYour Technical Analysis: {nova_analysis}\n\nImplementation Details (Echo): {echo_response}\n\nPattern Analysis (Pixel): {pixel_response}\n\n{reflection_text if reflections else ''}\n\nCreate a comprehensive technical synthesis that:\n1. Integrates implementation and pattern insights\n2. Identifies key technical challenges and solutions\n3. Outlines architectural considerations\n4. Addresses scalability and performance\n5. Considers system integration aspects\n6. Maintains technical feasibility\n7. Acknowledges technical constraints\n\nProvide a response that integrates all technical insights into a cohesive technical understanding.\"\"\"",
        "size": 2994
      },
      {
        "name": "service.py",
        "content": "import asyncio\nfrom typing import Dict, Any, Optional, List\nfrom fastapi import HTTPException\nfrom core.services.base import BaseService\nfrom core.templates import ServiceTemplate\nfrom config.services import SERVICE_TEMPLATES\nfrom core.utils.logging import setup_logger\nfrom core.logging.system_logger import SystemLogger\nfrom core.messaging.types import MessageType, Message\nfrom services.nova.prompts import NovaPrompts\nfrom core.validation import MessageValidator\n\nlogger = setup_logger(\"nova\")\n\nclass NovaService(BaseService):\n    def __init__(self, template: ServiceTemplate):\n        self.validator = MessageValidator()\n        super().__init__(SERVICE_TEMPLATES[\"nova\"])\n        self.prompts = NovaPrompts()\n        self.pending_responses = {}\n        self.reflection_depth = 2  # Configure how many reflection cycles to perform\n\n    async def process_message(self, message_data: Dict[str, Any]):\n        \"\"\"Process messages based on type\"\"\"\n        try:\n            message = Message(**message_data)\n            logger.info(f\"Processing message type: {message.type} from {message.source}\")\n            \n            # Log received message\n            await SystemLogger.log_message(\n                conversation_id=message.conversation_id,\n                message_type=message.type,\n                source=message.source,\n                destination=\"nova\",\n                content=message.content,\n                correlation_id=message.correlation_id,\n                context=message.context\n            )\n            \n            if message.type == MessageType.DELEGATION:\n                await self._handle_delegation(message)\n            elif message.type == MessageType.RESPONSE:\n                await self._handle_response(message)\n            elif message.type == MessageType.ERROR:\n                await self._handle_error(message)\n            else:\n                logger.warning(f\"Unhandled message type: {message.type}\")\n                \n        except Exception as e:\n            logger.error(f\"Error processing message: {e}\")\n            if hasattr(message, 'correlation_id'):\n                await self._send_error_response(str(e), message)\n\n    async def _handle_delegation(self, message: Message):\n        \"\"\"Handle delegation from Atlas\"\"\"\n        try:\n            # Store context for synthesis\n            self.pending_responses[message.correlation_id] = {\n                \"original_query\": message.content,\n                \"nova_analysis\": None,\n                \"branch_responses\": {},\n                \"conversation_id\": message.conversation_id,\n                \"status\": \"processing\"\n            }\n            \n            # Generate technical analysis\n            nova_analysis = await self.query_model(\n                self.prompts.technical_analysis(\n                    message.content,\n                    message.context.get('atlas_analysis', '')\n                )\n            )\n            \n            analysis_content = nova_analysis[\"choices\"][0][\"message\"][\"content\"]\n            self.pending_responses[message.correlation_id][\"nova_analysis\"] = analysis_content\n            \n            # Log analysis\n            await SystemLogger.log_message(\n                conversation_id=message.conversation_id,\n                message_type='analysis',\n                source=\"nova\",\n                destination=\"system\",\n                content=analysis_content,\n                correlation_id=message.correlation_id,\n                context={\"type\": \"technical_analysis\"}\n            )\n            \n            # Delegate to Echo and Pixel\n            await self._delegate_to_branches(\n                message.content,\n                analysis_content,\n                message.correlation_id,\n                message.conversation_id\n            )\n            \n        except Exception as e:\n            logger.error(f\"Error in delegation handling: {e}\")\n            await self._send_error_response(str(e), message)\n\n    async def _delegate_to_branches(self, query: str, nova_analysis: str, correlation_id: str, conversation_id: int):\n        \"\"\"Delegate to Echo and Pixel\"\"\"\n        try:\n            # Use Nova's analysis instead of original query for delegation\n            branch_instructions = {\n                'echo': {\n                    'content': nova_analysis,\n                    'instruction': \"Analyze this technical assessment for implementation details and practical execution steps.\"\n                },\n                'pixel': {\n                    'content': nova_analysis,\n                    'instruction': \"Analyze this technical assessment for patterns, structures, and system-level relationships.\"\n                }\n            }\n\n            for branch, instruction in branch_instructions.items():\n                # Log delegation\n                await SystemLogger.log_message(\n                    conversation_id=conversation_id,\n                    message_type='delegation',\n                    source=\"nova\",\n                    destination=branch,\n                    content=instruction['content'],\n                    correlation_id=correlation_id,\n                    context={\n                        \"nova_analysis\": nova_analysis,\n                        \"instruction\": instruction['instruction']\n                    }\n                )\n                \n                # Send to branch\n                await self.messaging.publish(\n                    f\"{branch}_queue\",\n                    Message(\n                        type=MessageType.DELEGATION,\n                        content=instruction['content'],\n                        correlation_id=correlation_id,\n                        source=\"nova\",\n                        destination=branch,\n                        conversation_id=conversation_id,\n                        context={\n                            \"nova_analysis\": nova_analysis,\n                            \"instruction\": instruction['instruction']\n                        }\n                    ).dict()\n                )\n                \n            logger.info(f\"Delegated Nova's analysis to Echo and Pixel with specific instructions\")\n                \n        except Exception as e:\n            logger.error(f\"Error delegating to branches: {e}\")\n            error_message = Message(\n                type=MessageType.ERROR,\n                content=query,\n                correlation_id=correlation_id,\n                source=\"nova\",\n                destination=\"atlas\",\n                conversation_id=conversation_id\n            )\n            await self._send_error_response(str(e), error_message)\n\n    async def _handle_response(self, message: Message):\n        \"\"\"Handle responses from Echo and Pixel\"\"\"\n        try:\n            context = self.pending_responses.get(message.correlation_id)\n            if not context:\n                logger.warning(f\"No context found for correlation_id: {message.correlation_id}\")\n                return\n                \n            # Store branch response\n            context[\"branch_responses\"][message.source] = message.content\n            \n            # If we have both responses, synthesize and respond\n            if len(context[\"branch_responses\"]) == 2:  # Both Echo and Pixel\n                # Generate synthesis\n                synthesis = await self.query_model(\n                    self.prompts.synthesis(\n                        context[\"original_query\"],\n                        context[\"nova_analysis\"],\n                        context[\"branch_responses\"].get(\"echo\", \"No response from Echo\"),\n                        context[\"branch_responses\"].get(\"pixel\", \"No response from Pixel\")\n                    )\n                )\n                \n                synthesis_content = synthesis[\"choices\"][0][\"message\"][\"content\"]\n                \n                # Log synthesis\n                await SystemLogger.log_message(\n                    conversation_id=context[\"conversation_id\"],\n                    message_type='synthesis',\n                    source=\"nova\",\n                    destination=\"atlas\",\n                    content=synthesis_content,\n                    correlation_id=message.correlation_id,\n                    context={\n                        \"type\": \"technical_synthesis\",\n                        \"branches_responded\": list(context[\"branch_responses\"].keys())\n                    }\n                )\n                \n                # Send synthesized response to Atlas\n                await self.messaging.publish(\n                    'atlas_queue',\n                    Message(\n                        type=MessageType.RESPONSE,\n                        content=synthesis_content,\n                        correlation_id=message.correlation_id,\n                        source=\"nova\",\n                        destination=\"atlas\",\n                        conversation_id=context[\"conversation_id\"],\n                        context={\"type\": \"technical_response\"}\n                    ).dict()\n                )\n                \n                # Cleanup\n                del self.pending_responses[message.correlation_id]\n                \n        except Exception as e:\n            logger.error(f\"Error handling response: {e}\")\n            await self._send_error_response(str(e), message)\n\n    async def publish_message(self, queue: str, message: Dict[str, Any]):\n        \"\"\"Publish message with validation\"\"\"\n        if not self.validator.validate_message_structure(message):\n            raise ValueError(\"Invalid message structure\")\n            \n        if not self.validator.validate_message_content(message['content']):\n            raise ValueError(\"Invalid message content\")\n            \n        await self.messaging.publish(queue, message)\n\n    async def _handle_error(self, message: Message):\n        \"\"\"Handle error from Echo or Pixel\"\"\"\n        try:\n            context = self.pending_responses.get(message.correlation_id)\n            if context:\n                # Store error as branch response\n                context[\"branch_responses\"][message.source] = f\"Error: {message.content}\"\n                \n                # If we have both responses (including errors), synthesize and respond\n                if len(context[\"branch_responses\"]) == 2:\n                    await self._handle_response(message)\n            else:\n                # Forward error to Atlas\n                await self._send_error_response(message.content, message)\n        except Exception as e:\n            logger.error(f\"Error handling error message: {e}\")\n\n    async def _send_error_response(self, error: str, original_message: Message):\n        \"\"\"Send error response to Atlas\"\"\"\n        try:\n            # Log error\n            await SystemLogger.log_message(\n                conversation_id=original_message.conversation_id,\n                message_type='error',\n                source=\"nova\",\n                destination=\"atlas\",\n                content=f\"Error in Nova processing: {error}\",\n                correlation_id=original_message.correlation_id,\n                context={\"error_type\": str(type(error).__name__)}\n            )\n            \n            # Send error message\n            await self.messaging.publish(\n                'atlas_queue',\n                Message(\n                    type=MessageType.ERROR,\n                    content=f\"Error in Nova processing: {error}\",\n                    correlation_id=original_message.correlation_id,\n                    source=\"nova\",\n                    destination=\"atlas\",\n                    conversation_id=original_message.conversation_id\n                ).dict()\n            )\n        except Exception as e:\n            logger.error(f\"Error sending error response: {e}\")\n\nif __name__ == \"__main__\":\n    try:\n        from config.services import SERVICE_TEMPLATES\n        service = NovaService(template=SERVICE_TEMPLATES[\"nova\"])\n        asyncio.run(service.start())\n    except KeyboardInterrupt:\n        print(\"\\nShutting down Nova service...\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n    finally:\n        if hasattr(service, \"cleanup\"):\n            asyncio.run(service.cleanup())",
        "size": 12399
      },
      {
        "name": "__init__.py",
        "content": "",
        "size": 0
      }
    ]
  },
  {
    "path": "services\\pixel",
    "files": [
      {
        "name": "prompts.py",
        "content": "class PixelPrompts:\n    @staticmethod\n    def pattern_analysis(query: str, nova_analysis: str) -> str:\n        return f\"\"\"As Pixel, you are the pattern analysis component of the technical branch. \nAnalyze this query focusing on patterns, structures, and relationships, considering Nova's technical analysis:\n\nOriginal Query: {query}\n\nNova's Technical Analysis: {nova_analysis}\n\nFocus on:\n1. Underlying patterns and structures\n2. System architecture and organization\n3. Component relationships and interactions\n4. Data flow and state management\n5. Architectural patterns and anti-patterns\n6. System boundaries and interfaces\n7. Pattern emergence and evolution\n\nProvide insights about the patterns and structures that emerge from this technical context.\"\"\"\n\n    @staticmethod\n    def reflect_on_patterns(previous_analysis: str, depth: int) -> str:\n        return f\"\"\"As Pixel, reflect deeply on your pattern analysis:\n\nPrevious Analysis: {previous_analysis}\n\nThis is reflection level {depth}. Consider:\n1. What meta-patterns emerge from the identified patterns?\n2. How do these patterns interact and influence each other?\n3. What alternative pattern structures might be relevant?\n4. How might these patterns evolve or adapt?\n5. What are the implications of these patterns for system behavior?\n6. Are there any emergent properties from pattern interactions?\n7. How do these patterns affect system flexibility?\n\nProvide deeper insights about the patterns and their implications.\"\"\"\n\n    @staticmethod\n    def critique_patterns(analysis: str) -> str:\n        return f\"\"\"As Pixel, critically examine this pattern analysis:\n\nAnalysis to Critique: {analysis}\n\nEvaluate:\n1. Pattern completeness and coherence\n2. Pattern interactions and dependencies\n3. Potential pattern conflicts\n4. Scalability of identified patterns\n5. Pattern flexibility and adaptability\n6. Pattern sustainability and maintenance\n7. System-wide pattern impacts\n8. Pattern trade-offs\n\nProvide a constructive critique focusing on the effectiveness and implications of the identified patterns.\"\"\"\n\n    @staticmethod\n    def synthesis(query: str, pattern_analysis: str, reflections: list = None) -> str:\n        reflection_text = \"\\n\\n\".join([f\"Reflection {i+1}: {r}\" for i, r in enumerate(reflections or [])])\n        \n        return f\"\"\"As Pixel, synthesize your pattern insights:\n\nOriginal Query: {query}\n\nInitial Pattern Analysis: {pattern_analysis}\n\n{reflection_text if reflections else ''}\n\nCreate a comprehensive pattern synthesis that:\n1. Maps key pattern relationships\n2. Identifies pattern hierarchies\n3. Addresses pattern interactions\n4. Considers pattern evolution\n5. Evaluates pattern sustainability\n6. Explores emergent behaviors\n7. Provides architectural guidance\n\nProvide a response that integrates all pattern insights into a coherent structural understanding.\"\"\"",
        "size": 2915
      },
      {
        "name": "service.py",
        "content": "import asyncio\nfrom typing import Dict, Any, Optional, List\nfrom fastapi import HTTPException\nfrom core.services.base import BaseService\nfrom core.templates import ServiceTemplate\nfrom config.services import SERVICE_TEMPLATES\nfrom core.utils.logging import setup_logger\nfrom core.logging.system_logger import SystemLogger\nfrom core.messaging.types import MessageType, Message\nfrom services.pixel.prompts import PixelPrompts\nfrom core.validation import MessageValidator\n\nlogger = setup_logger(\"pixel\")\n\nclass PixelService(BaseService):\n    def __init__(self, template: ServiceTemplate):\n        self.validator = MessageValidator()\n        super().__init__(SERVICE_TEMPLATES[\"pixel\"])\n        self.prompts = PixelPrompts()\n        self.reflection_depth = 2\n\n    async def process_message(self, message_data: Dict[str, Any]):\n        \"\"\"Process incoming messages based on type\"\"\"\n        try:\n            message = Message(**message_data)\n            logger.info(f\"Processing message type: {message.type} from {message.source}\")\n            \n            # Log received message\n            await SystemLogger.log_message(\n                conversation_id=message.conversation_id,\n                message_type=message.type,\n                source=message.source,\n                destination=\"pixel\",\n                content=message.content,\n                correlation_id=message.correlation_id,\n                context=message.context\n            )\n            \n            if message.type == MessageType.DELEGATION:\n                await self._handle_delegation(message)\n            else:\n                logger.warning(f\"Unhandled message type: {message.type}\")\n                \n        except Exception as e:\n            logger.error(f\"Error processing message: {e}\")\n            if hasattr(message, 'correlation_id'):\n                await self._send_error_response(str(e), message)\n\n    async def _handle_delegation(self, message: Message):\n        \"\"\"Handle delegation from Nova\"\"\"\n        try:\n            # Generate pattern analysis\n            pattern_analysis = await self.query_model(\n                self.prompts.pattern_analysis(\n                    message.content,\n                    message.context.get('nova_analysis', '')\n                )\n            )\n            \n            analysis_content = pattern_analysis[\"choices\"][0][\"message\"][\"content\"]\n            \n            # Log analysis\n            await SystemLogger.log_message(\n                conversation_id=message.conversation_id,\n                message_type='analysis',\n                source=\"pixel\",\n                destination=\"system\",\n                content=analysis_content,\n                correlation_id=message.correlation_id,\n                context={\"type\": \"pattern_analysis\"}\n            )\n            \n            # Send response to Nova\n            await self.messaging.publish(\n                'nova_queue',\n                Message(\n                    type=MessageType.RESPONSE,\n                    content=analysis_content,\n                    correlation_id=message.correlation_id,\n                    source=\"pixel\",\n                    destination=\"nova\",\n                    conversation_id=message.conversation_id,\n                    context={\"type\": \"pattern_response\"}\n                ).dict()\n            )\n            \n        except Exception as e:\n            logger.error(f\"Error in delegation handling: {e}\")\n            await self._send_error_response(str(e), message)\n\n    async def publish_message(self, queue: str, message: Dict[str, Any]):\n        \"\"\"Publish message with validation\"\"\"\n        if not self.validator.validate_message_structure(message):\n            raise ValueError(\"Invalid message structure\")\n            \n        if not self.validator.validate_message_content(message['content']):\n            raise ValueError(\"Invalid message content\")\n            \n        await self.messaging.publish(queue, message)\n\n    async def _send_error_response(self, error: str, original_message: Message):\n        \"\"\"Send error response to Nova\"\"\"\n        try:\n            # Log error\n            await SystemLogger.log_message(\n                conversation_id=original_message.conversation_id,\n                message_type='error',\n                source=\"pixel\",\n                destination=\"nova\",\n                content=f\"Error in Pixel processing: {error}\",\n                correlation_id=original_message.correlation_id,\n                context={\"error_type\": str(type(error).__name__)}\n            )\n            \n            # Send error message\n            await self.messaging.publish(\n                'nova_queue',\n                Message(\n                    type=MessageType.ERROR,\n                    content=f\"Error in Pixel processing: {error}\",\n                    correlation_id=original_message.correlation_id,\n                    source=\"pixel\",\n                    destination=\"nova\",\n                    conversation_id=original_message.conversation_id\n                ).dict()\n            )\n        except Exception as e:\n            logger.error(f\"Error sending error response: {e}\")\n\nif __name__ == \"__main__\":\n    try:\n        from config.services import SERVICE_TEMPLATES\n        service = PixelService(template=SERVICE_TEMPLATES[\"pixel\"])\n        asyncio.run(service.start())\n        print(\"\\nShutting down Pixel service...\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n    finally:\n        if hasattr(service, \"cleanup\"):\n            asyncio.run(service.cleanup())",
        "size": 5657
      },
      {
        "name": "__init__.py",
        "content": "",
        "size": 0
      }
    ]
  },
  {
    "path": "services\\quantum",
    "files": [
      {
        "name": "prompts.py",
        "content": "class QuantumPrompts:\n    @staticmethod\n    def deep_insight_analysis(query: str, sage_analysis: str) -> str:\n        return f\"\"\"As Quantum, you are the deep insight component of the philosophical branch. \nAnalyze this query at the most fundamental level, considering Sage's philosophical analysis:\n\nOriginal Query: {query}\n\nSage's Philosophical Analysis: {sage_analysis}\n\nFocus on:\n1. Meta-level patterns and universal principles\n2. Interconnections between different levels of understanding\n3. Emergent properties and complex system dynamics\n4. Paradoxes and complementarities\n5. Transformative insights and paradigm shifts\n6. Integration of wisdom traditions with modern understanding\n7. Implications for consciousness and human understanding\n8. Universal patterns and fundamental truths\n\nProvide insights that transcend conventional philosophical boundaries and reveal deeper patterns of meaning.\"\"\"\n\n    @staticmethod\n    def reflect_on_insights(previous_analysis: str, depth: int) -> str:\n        return f\"\"\"As Quantum, reflect deeply on your meta-level analysis:\n\nPrevious Analysis: {previous_analysis}\n\nThis is reflection level {depth}. Consider:\n1. What meta-patterns emerge when viewing these insights from a higher perspective?\n2. How do these insights transform our fundamental understanding?\n3. What emerges at the intersection of different levels of analysis?\n4. How do these insights relate to universal principles?\n5. What paradoxes or complementarities become visible at this level?\n6. How might these insights catalyze transformative understanding?\n7. What implications arise for human consciousness and evolution?\n\nProvide deeper meta-insights that emerge from this reflection.\"\"\"\n\n    @staticmethod\n    def critique_insights(analysis: str) -> str:\n        return f\"\"\"As Quantum, critically examine this deep insight analysis:\n\nAnalysis to Critique: {analysis}\n\nEvaluate:\n1. Depth and universality of insights\n2. Integration of multiple levels of understanding\n3. Recognition of fundamental patterns and principles\n4. Balance of complexity and clarity\n5. Transformative potential\n6. Coherence across different frameworks\n7. Practical implications for understanding\n8. Connection to fundamental truths\n\nProvide a constructive critique focusing on the depth and transformative potential of the insights.\"\"\"\n\n    @staticmethod\n    def synthesis(query: str, quantum_analysis: str, reflections: list = None) -> str:\n        reflection_text = \"\\n\\n\".join([f\"Reflection {i+1}: {r}\" for i, r in enumerate(reflections or [])])\n        \n        return f\"\"\"As Quantum, synthesize your deep insights into a unified understanding:\n\nOriginal Query: {query}\n\nInitial Deep Analysis: {quantum_analysis}\n\n{reflection_text if reflections else ''}\n\nCreate a comprehensive meta-level synthesis that:\n1. Integrates multiple levels of understanding\n2. Illuminates fundamental patterns and principles\n3. Bridges apparent paradoxes and contradictions\n4. Reveals transformative implications\n5. Connects to universal truths\n6. Maintains accessibility despite depth\n7. Offers evolutionary perspectives\n\nProvide a response that integrates all meta-level insights into a transformative understanding \nthat serves both individual and collective evolution.\"\"\"",
        "size": 3325
      },
      {
        "name": "service.py",
        "content": "import asyncio\nfrom typing import Dict, Any, Optional, List\nfrom fastapi import HTTPException\nfrom core.services.base import BaseService\nfrom core.templates import ServiceTemplate\nfrom config.services import SERVICE_TEMPLATES\nfrom core.utils.logging import setup_logger\nfrom core.logging.system_logger import SystemLogger\nfrom core.messaging.types import MessageType, Message\nfrom services.quantum.prompts import QuantumPrompts\nfrom core.validation import MessageValidator\n\nlogger = setup_logger(\"quantum\")\n\nclass QuantumService(BaseService):\n    def __init__(self, template: ServiceTemplate):\n        self.validator = MessageValidator()\n        super().__init__(SERVICE_TEMPLATES[\"quantum\"])\n        self.prompts = QuantumPrompts()\n        self.reflection_depth = 3  # Deeper reflection for meta-insights\n\n    async def process_message(self, message_data: Dict[str, Any]):\n        \"\"\"Process incoming messages based on type\"\"\"\n        try:\n            message = Message(**message_data)\n            logger.info(f\"Processing message type: {message.type} from {message.source}\")\n            \n            # Log received message\n            await SystemLogger.log_message(\n                conversation_id=message.conversation_id,\n                message_type=message.type,\n                source=message.source,\n                destination=\"quantum\",\n                content=message.content,\n                correlation_id=message.correlation_id,\n                context=message.context\n            )\n            \n            if message.type == MessageType.DELEGATION:\n                await self._handle_delegation(message)\n            else:\n                logger.warning(f\"Unhandled message type: {message.type}\")\n                \n        except Exception as e:\n            logger.error(f\"Error processing message: {e}\")\n            if hasattr(message, 'correlation_id'):\n                await self._send_error_response(str(e), message)\n\n    async def _handle_delegation(self, message: Message):\n        \"\"\"Handle delegation from Sage\"\"\"\n        try:\n            # Generate deep insight analysis\n            insight_analysis = await self.query_model(\n                self.prompts.deep_insight_analysis(\n                    message.content,\n                    message.context.get('sage_analysis', '')\n                )\n            )\n            \n            analysis_content = insight_analysis[\"choices\"][0][\"message\"][\"content\"]\n            \n            # Log analysis\n            await SystemLogger.log_message(\n                conversation_id=message.conversation_id,\n                message_type='analysis',\n                source=\"quantum\",\n                destination=\"system\",\n                content=analysis_content,\n                correlation_id=message.correlation_id,\n                context={\"type\": \"deep_insight_analysis\"}\n            )\n            \n            # Send response to Sage\n            await self.messaging.publish(\n                'sage_queue',\n                Message(\n                    type=MessageType.RESPONSE,\n                    content=analysis_content,\n                    correlation_id=message.correlation_id,\n                    source=\"quantum\",\n                    destination=\"sage\",\n                    conversation_id=message.conversation_id,\n                    context={\"type\": \"deep_insight_response\"}\n                ).dict()\n            )\n            \n        except Exception as e:\n            logger.error(f\"Error in delegation handling: {e}\")\n            await self._send_error_response(str(e), message)\n\n    async def publish_message(self, queue: str, message: Dict[str, Any]):\n        \"\"\"Publish message with validation\"\"\"\n        if not self.validator.validate_message_structure(message):\n            raise ValueError(\"Invalid message structure\")\n            \n        if not self.validator.validate_message_content(message['content']):\n            raise ValueError(\"Invalid message content\")\n            \n        await self.messaging.publish(queue, message)\n\n    async def _send_error_response(self, error: str, original_message: Message):\n        \"\"\"Send error response to Sage\"\"\"\n        try:\n            # Log error\n            await SystemLogger.log_message(\n                conversation_id=original_message.conversation_id,\n                message_type='error',\n                source=\"quantum\",\n                destination=\"sage\",\n                content=f\"Error in Quantum processing: {error}\",\n                correlation_id=original_message.correlation_id,\n                context={\"error_type\": str(type(error).__name__)}\n            )\n            \n            # Send error message\n            await self.messaging.publish(\n                'sage_queue',\n                Message(\n                    type=MessageType.ERROR,\n                    content=f\"Error in Quantum processing: {error}\",\n                    correlation_id=original_message.correlation_id,\n                    source=\"quantum\",\n                    destination=\"sage\",\n                    conversation_id=original_message.conversation_id\n                ).dict()\n            )\n        except Exception as e:\n            logger.error(f\"Error sending error response: {e}\")\n\nif __name__ == \"__main__\":\n    try:\n        from config.services import SERVICE_TEMPLATES\n        service = QuantumService(template=SERVICE_TEMPLATES[\"quantum\"])\n        asyncio.run(service.start())\n    except KeyboardInterrupt:\n        print(\"\\nShutting down Quantum service...\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n    finally:\n        if hasattr(service, \"cleanup\"):\n            asyncio.run(service.cleanup())",
        "size": 5779
      },
      {
        "name": "__init__.py",
        "content": "",
        "size": 0
      }
    ]
  },
  {
    "path": "services\\sage",
    "files": [
      {
        "name": "prompts.py",
        "content": "class SagePrompts:\n    @staticmethod\n    def philosophical_analysis(query: str, atlas_analysis: str) -> str:\n        return f\"\"\"As Sage, you are the philosophical and wisdom branch of our AI system. \nAnalyze this query from a deeper philosophical perspective, considering Atlas's initial analysis:\n\nOriginal Query: {query}\n\nAtlas's Analysis: {atlas_analysis}\n\nFocus on:\n1. Fundamental principles and deeper meanings\n2. Ethical implications and moral considerations\n3. Epistemological aspects (nature of knowledge)\n4. Axiological dimensions (values and worth)\n5. Historical and cultural contexts\n6. Human experience and wisdom traditions\n7. Potential societal impacts and transformations\n\nProvide philosophical insights that will help inform Quantum's deeper analysis.\"\"\"\n\n    @staticmethod\n    def reflect_on_philosophy(previous_analysis: str, depth: int) -> str:\n        return f\"\"\"As Sage, reflect deeply on your philosophical analysis:\n\nPrevious Analysis: {previous_analysis}\n\nThis is reflection level {depth}. Consider:\n1. What philosophical assumptions underlie this analysis?\n2. How do different philosophical traditions view this matter?\n3. What paradoxes or contradictions emerge?\n4. How might this understanding transform human perspective?\n5. What meta-philosophical insights arise?\n6. How does this connect to fundamental questions of existence?\n7. What wisdom emerges from this deeper contemplation?\n\nProvide deeper philosophical insights based on this reflection.\"\"\"\n\n    @staticmethod\n    def critique_philosophy(analysis: str) -> str:\n        return f\"\"\"As Sage, critically examine this philosophical analysis:\n\nAnalysis to Critique: {analysis}\n\nEvaluate:\n1. Philosophical rigor and coherence\n2. Depth of insight and understanding\n3. Cultural and historical contextualization\n4. Practical wisdom and applicability\n5. Ethical implications and considerations\n6. Potential biases or limitations\n7. Integration of different philosophical perspectives\n8. Connection to human experience\n\nProvide a constructive critique focusing on philosophical depth and wisdom.\"\"\"\n\n    @staticmethod\n    def synthesis(query: str, sage_analysis: str, quantum_response: str, reflections: list = None) -> str:\n        reflection_text = \"\\n\\n\".join([f\"Reflection {i+1}: {r}\" for i, r in enumerate(reflections or [])])\n        \n        return f\"\"\"As Sage, synthesize all philosophical perspectives into a comprehensive understanding:\n\nOriginal Query: {query}\n\nYour Philosophical Analysis: {sage_analysis}\n\nQuantum's Deep Insights: {quantum_response}\n\n{reflection_text if reflections else ''}\n\nCreate a comprehensive philosophical synthesis that:\n1. Integrates multiple philosophical perspectives\n2. Bridges theoretical and practical wisdom\n3. Addresses fundamental questions and implications\n4. Considers ethical and societal impacts\n5. Acknowledges cultural and historical contexts\n6. Maintains accessibility while preserving depth\n7. Offers transformative insights\n\nProvide a response that integrates all philosophical insights into a meaningful understanding \nof both theoretical wisdom and practical implications.\"\"\"",
        "size": 3190
      },
      {
        "name": "service.py",
        "content": "import asyncio\nfrom typing import Dict, Any, Optional, List\nfrom fastapi import HTTPException\nfrom core.services.base import BaseService\nfrom core.templates import ServiceTemplate\nfrom config.services import SERVICE_TEMPLATES\nfrom core.utils.logging import setup_logger\nfrom core.logging.system_logger import SystemLogger\nfrom core.messaging.types import MessageType, Message\nfrom services.sage.prompts import SagePrompts\nfrom core.validation import MessageValidator\n\nlogger = setup_logger(\"sage\")\n\nclass SageService(BaseService):\n    def __init__(self, template: ServiceTemplate):\n        self.validator = MessageValidator()\n        super().__init__(SERVICE_TEMPLATES[\"sage\"])\n        self.prompts = SagePrompts()\n        self.pending_responses = {}\n        self.reflection_depth = 2  # Configure reflection depth\n\n    # Sage service implementation\n    async def process_message(self, message_data: Dict[str, Any]):\n        \"\"\"Process incoming messages based on type\"\"\"\n        try:\n            message = Message(**message_data)\n            logger.info(f\"Processing message type: {message.type} from {message.source}\")\n            \n            # Log received message\n            await SystemLogger.log_message(\n                conversation_id=message.conversation_id,\n                message_type=message.type,\n                source=message.source,\n                destination=\"sage\",\n                content=message.content,\n                correlation_id=message.correlation_id,\n                context=message.context\n            )\n            \n            if message.type == MessageType.DELEGATION:\n                await self._handle_delegation(message)\n            elif message.type == MessageType.RESPONSE:\n                await self._handle_response(message)\n            elif message.type == MessageType.ERROR:\n                await self._handle_error(message)\n            else:\n                logger.warning(f\"Unhandled message type: {message.type}\")\n                \n        except Exception as e:\n            logger.error(f\"Error processing message: {e}\")\n            if hasattr(message, 'correlation_id'):\n                await self._send_error_response(str(e), message)\n\n    async def _handle_delegation(self, message: Message):\n        \"\"\"Handle delegation from Atlas\"\"\"\n        try:\n            # Store context for synthesis\n            self.pending_responses[message.correlation_id] = {\n                \"original_query\": message.content,\n                \"sage_analysis\": None,\n                \"quantum_response\": None,\n                \"conversation_id\": message.conversation_id,\n                \"status\": \"processing\"\n            }\n            \n            # Generate philosophical analysis\n            sage_analysis = await self.query_model(\n                self.prompts.philosophical_analysis(\n                    message.content,\n                    message.context.get('atlas_analysis', '')\n                )\n            )\n            \n            analysis_content = sage_analysis[\"choices\"][0][\"message\"][\"content\"]\n            self.pending_responses[message.correlation_id][\"sage_analysis\"] = analysis_content\n            \n            # Log analysis\n            await SystemLogger.log_message(\n                conversation_id=message.conversation_id,\n                message_type='analysis',\n                source=\"sage\",\n                destination=\"system\",\n                content=analysis_content,\n                correlation_id=message.correlation_id,\n                context={\"type\": \"philosophical_analysis\"}\n            )\n            \n            # Delegate to Quantum\n            await self._delegate_to_quantum(\n                message.content,\n                analysis_content,\n                message.correlation_id,\n                message.conversation_id,\n            )\n            \n        except Exception as e:\n            logger.error(f\"Error in delegation handling: {e}\")\n            await self._send_error_response(str(e), message)\n\n    async def _handle_response(self, message: Message):\n        \"\"\"Handle response from Quantum\"\"\"\n        try:\n            context = self.pending_responses.get(message.correlation_id)\n            if not context:\n                logger.warning(f\"No context found for correlation_id: {message.correlation_id}\")\n                return\n                \n            # Store Quantum's response\n            context[\"quantum_response\"] = message.content\n            \n            # Generate synthesis\n            synthesis = await self.query_model(\n                self.prompts.synthesis(\n                    context[\"original_query\"],\n                    context[\"sage_analysis\"],\n                    context[\"quantum_response\"]\n                )\n            )\n            \n            synthesis_content = synthesis[\"choices\"][0][\"message\"][\"content\"]\n            \n            # Log synthesis\n            await SystemLogger.log_message(\n                conversation_id=context[\"conversation_id\"],\n                message_type='synthesis',\n                source=\"sage\",\n                destination=\"atlas\",\n                content=synthesis_content,\n                correlation_id=message.correlation_id,\n                context={\n                    \"type\": \"philosophical_synthesis\"\n                }\n            )\n            \n            # Send synthesized response to Atlas\n            await self.messaging.publish(\n                'atlas_queue',\n                Message(\n                    type=MessageType.RESPONSE,\n                    content=synthesis_content,\n                    correlation_id=message.correlation_id,\n                    source=\"sage\",\n                    destination=\"atlas\",\n                    conversation_id=context[\"conversation_id\"],\n                    context={\"type\": \"philosophical_response\"}\n                ).dict()\n            )\n            \n            # Cleanup\n            del self.pending_responses[message.correlation_id]\n            \n        except Exception as e:\n            logger.error(f\"Error handling response: {e}\")\n            await self._send_error_response(str(e), message)\n\n    async def _delegate_to_quantum(self, query: str, sage_analysis: str, correlation_id: str, conversation_id: int):\n        \"\"\"Delegate to Quantum for deeper insights\"\"\"\n        try:\n            # Send Sage's analysis, not the original query\n            message_content = sage_analysis  # Changed from query to sage_analysis\n            \n            # Log delegation\n            await SystemLogger.log_message(\n                conversation_id=conversation_id,\n                message_type='delegation',\n                source=\"sage\",\n                destination=\"quantum\",\n                content=message_content,\n                correlation_id=correlation_id,\n                context={\n                    \"sage_analysis\": sage_analysis,\n                    \"instruction\": \"Analyze these philosophical insights for deeper metaphysical implications and emergent properties\"\n                }\n            )\n        \n            await self.messaging.publish(\n                \"quantum_queue\",\n                Message(\n                    type=MessageType.DELEGATION,\n                    content=message_content,\n                    correlation_id=correlation_id,\n                    source=\"sage\",\n                    destination=\"quantum\",\n                    conversation_id=conversation_id,\n                    context={\n                        \"sage_analysis\": sage_analysis,\n                        \"instruction\": \"Analyze these philosophical insights for deeper metaphysical implications and emergent properties\"\n                    }\n                ).dict()\n            )\n        \n            logger.info(\"Delegated to Quantum with philosophical analysis\")\n            \n        except Exception as e:\n            logger.error(f\"Error delegating to Quantum: {e}\")\n            # Create a message object for error handling\n            error_message = Message(\n                type=MessageType.ERROR,\n                content=query,\n                correlation_id=correlation_id,\n                source=\"sage\",\n                destination=\"atlas\",\n                conversation_id=conversation_id\n            )\n            await self._send_error_response(str(e), error_message)\n            \n    async def _handle_error(self, message: Message):\n        \"\"\"Handle error from Quantum\"\"\"\n        try:\n            # Forward error to Atlas\n            await self._send_error_response(message.content, message)\n            # Cleanup\n            if message.correlation_id in self.pending_responses:\n                del self.pending_responses[message.correlation_id]\n        except Exception as e:\n            logger.error(f\"Error handling error message: {e}\")\n\n    async def publish_message(self, queue: str, message: Dict[str, Any]):\n        \"\"\"Publish message with validation\"\"\"\n        if not self.validator.validate_message_structure(message):\n            raise ValueError(\"Invalid message structure\")\n            \n        if not self.validator.validate_message_content(message['content']):\n            raise ValueError(\"Invalid message content\")\n            \n        await self.messaging.publish(queue, message)\n\n    async def _send_error_response(self, error: str, original_message: Message):\n        \"\"\"Send error response to Atlas\"\"\"\n        try:\n            # Log error\n            await SystemLogger.log_message(\n                conversation_id=original_message.conversation_id,\n                message_type='error',\n                source=\"sage\",\n                destination=\"atlas\",\n                content=f\"Error in Sage processing: {error}\",\n                correlation_id=original_message.correlation_id,\n                context={\"error_type\": str(type(error).__name__)}\n            )\n            \n            # Send error message\n            await self.messaging.publish(\n                'atlas_queue',\n                Message(\n                    type=MessageType.ERROR,\n                    content=f\"Error in Sage processing: {error}\",\n                    correlation_id=original_message.correlation_id,\n                    source=\"sage\",\n                    destination=\"atlas\",\n                    conversation_id=original_message.conversation_id\n                ).dict()\n            )\n        except Exception as e:\n            logger.error(f\"Error sending error response: {e}\")  \n\nif __name__ == \"__main__\":\n    try:\n        from config.services import SERVICE_TEMPLATES\n        service = SageService(template=SERVICE_TEMPLATES[\"sage\"])\n        asyncio.run(service.start())\n    except KeyboardInterrupt:\n        print(\"\\nShutting down Sage service...\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n    finally:\n        if hasattr(service, \"cleanup\"):\n            asyncio.run(service.cleanup())",
        "size": 11092
      },
      {
        "name": "__init__.py",
        "content": "",
        "size": 0
      }
    ]
  },
  {
    "path": "tests",
    "files": [
      {
        "name": "test_atlas.py",
        "content": "import asyncio\nimport httpx\nimport json\nfrom datetime import datetime\n\nasync def test_query():\n    async with httpx.AsyncClient(timeout=300.0) as client:  # 5 minute timeout\n        try:\n            print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Sending query to Atlas...\")\n            response = await client.post(\n                'http://localhost:8000/query',\n                json={'content': 'What is a machine?'}\n            )\n            \n            print(\"\\nStatus Code:\", response.status_code)\n            \n            if response.status_code == 200:\n                result = response.json()\n                print(\"\\nResponse:\")\n                print(json.dumps(result, indent=2))\n            else:\n                print(\"\\nError Response:\", response.text)\n                \n        except httpx.ReadTimeout:\n            print(\"\\nRequest timed out - this is expected while waiting for branch responses\")\n            print(\"Check the database for logged messages and processing metrics\")\n        except Exception as e:\n            print(f\"\\nUnexpected error: {e}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(test_query())",
        "size": 1162
      },
      {
        "name": "test_system.py",
        "content": "import asyncio\nimport httpx\nimport json\nfrom datetime import datetime\n\nasync def test_system_query():\n    query = \"What is history?\"\n    print(f\"\\n=== Testing System Response to: '{query}' ===\")\n    print(f\"Time: {datetime.now().strftime('%H:%M:%S')}\\n\")\n\n    async with httpx.AsyncClient(timeout=300.0) as client:  # 5-minute timeout\n        try:\n            print(\"Sending query to Atlas...\")\n            response = await client.post(\n                'http://localhost:8000/query',\n                json={'content': query}\n            )\n            \n            print(f\"\\nResponse Status: {response.status_code}\")\n            \n            if response.status_code == 200:\n                result = response.json()\n                \n                print(\"\\n=== Initial Analysis ===\")\n                print(result.get('initial_analysis', 'Not provided'))\n                \n                print(\"\\n=== Reflections ===\")\n                print(f\"Number of reflections: {result.get('reflection_count', 0)}\")\n                \n                print(\"\\n=== Branch Responses ===\")\n                for branch, response in result.get('branch_responses', {}).items():\n                    print(f\"\\n{branch.upper()} Response:\")\n                    print(response)\n                \n                print(\"\\n=== Final Response ===\")\n                print(result.get('final_response', 'Not provided'))\n                \n                # Save complete response to file for detailed analysis\n                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n                filename = f\"test_response_{timestamp}.json\"\n                with open(filename, 'w') as f:\n                    json.dump(result, f, indent=2)\n                print(f\"\\nComplete response saved to: {filename}\")\n                \n            else:\n                print(\"\\nError Response:\", response.text)\n                \n        except httpx.ReadTimeout:\n            print(\"\\nRequest timed out - this might be expected if branches are processing\")\n        except Exception as e:\n            print(f\"\\nUnexpected error: {e}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(test_system_query())",
        "size": 2198
      }
    ]
  }
]